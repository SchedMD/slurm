<!--#include virtual="header.txt"-->

<h1>SLURM User and Administrator Guide for Cray systems</h1>

<h2>User Guide</h2>

<p>This document describes the unique features of SLURM on Cray computers.
You should be familiar with the SLURM's mode of operation on Linux clusters
before studying the differences in Cray system operation described in this
document.</p>

<p>SLURM version 2.3 is designed to operate as a job scheduler over Cray's
Application Level Placement Scheduler (ALPS).
Use SLURM's <i>sbatch</i> or <i>salloc</i> commands to create a resource
allocation in ALPS.
Then use ALPS' <i>aprun</i> command to launch parallel jobs within the resource
allocation.
The resource allocation is terminated once the the batch script or the
<i>salloc</i> command terminates.
Alternately there is an <i>aprun</i> wrapper distributed with SLURM in
<i>contribs/cray/srun</i> which will translate <i>srun</i> options
into the equivalent <i>aprun</i> options. This wrapper will also execute
<i>salloc</i> as needed to create a job allocation in which to run the
<i>aprun</i> command. The <i>srun</i> script contains two new options:
<i>--man</i> will print a summary of the options including notes about which
<i>srun</i> options are not supported and <i>--alps="</i> which can be used
to specify <i>aprun</i> options which lack an equivalent within <i>srun</i>.
For example, <i>srun --alps="-a xt" -n 4 a.out</i>.
Since <i>aprun</i> is used to launch tasks (the equivalent of a SLURM
job step), the job steps will not be visible using SLURM commands.
Other than SLURM's <i>srun</i> command being replaced by <i>aprun</i>
and the job steps not being visible, all other SLURM commands will operate
as expected. Note that in order to build and install the aprun wrapper
described above, execute "configure" with the <i>--with-srun2aprun</i>
option.</p>

<p>SLURM node names will be of the form "nid#####" where "#####" is a five
digit sequence number.
Other information available about the node is it's XYZ coordinate in the
node's <i>NodeAddr</i> field and it's component label in the <i>HostNodeName</i>
field.
The format of the component lable is "c#-#c#s#n#" where the "#" fields
represent in order: cabinet, row, cate, blade or slot, and node.
For example "c0-1c2s5n3" is cabinet 0, row 1, cage 3, slot 5 and node 3.</p>

<h2>Administrator Guide</h2>

<h3>Install supporting rpms</h3>

<p>The build requires a few -devel RPMs listed below. You can obtain these from
SuSe/Novel.
<ul>
<li>CLE 2.x uses SuSe SLES 10 packages (rpms may be on the normal isos)</li>
<li>CLE 3.x uses Suse SLES 11 packages (rpms are on the SDK isos, there
are two SDK iso files for SDK)</li>
</ul></p>

<p>You can check by
<ul>
<li>Going onto the bootnode</li>
<li>xtopview</li>
<li>rpm -qa</li>
</ul></p>

<p>The list of packages that should be installed is:
<ul>
<li>expat-2.0.xxx</li>
<li>libexpat-devel-2.0.xxx</li>
<li>cray-MySQL-devel-enterprise-5.0.64 (this should be on the Cray iso)</li>
</ul></p>

<p>ALl Cray-specific PrgEnv and compiler modules should be removed and root
privileges will be required to install these files.</p>

<h3>Create a build root</h3>

<p>The build is done on a normal service node, where you like
(e.g. <i>/ufs/slurm/build</i> would work).
Most scripts check for the environment variable LIBROOT. 
You can either edit the scripts or export this variable. Easiest way:
<ul>
<li>export LIBROOT=/ufs/slurm/build</li>
<li>mkdir -vp $LIBROOT</li>
<li>cd $LIBROOT</li>
</ul></p>

<h3>Install modulefile</h3>

<p>This file is distributed as part the SLURM tar-ball in
<i>contribs/cray/opt_modulefiles_slurm</i>. Install it as
<i>/opt/modulefiles/slurm</i> (or anywhere else in your module path).
It means that you can use Munge as soon as it is built.</p>

<h3>Build and install Munge</h3>

<p>Munge is the authentication daemon and needed by SLURM.
<ul>
<li>cp munge_build_script.sh $LIBROOT</li>
<li>mkdir -vp ${LIBROOT}/munge/zip</li>
<li>Download munge-0.5.9.tar.bz2 or newer from
<a href="http://code.google.com/p/munge/downloads/list">
http://code.google.com/p/munge/downloads/list</a></li>
<li>Copy that into  <i>${LIBROOT}/munge/zip</i></li>
<li>Execute <i>./munge_build_script.sh</i></li>
</ul></p>
<p>This generates a tar-ball called <i>$LIBROOT/munge_build-.*YYYY-MM-DD.tar.gz</i>
which has already the correct paths - in particular, the libmunge.* need to
reside in <i>/usr/lib64</i> since SLURM plugins call them.</p>

<p>Install the tar-ball by executing:
<ul>
<li>scp $LIBROOT/munge_build-.*YYYY-MM-DD.tar.gz boot:</li>
<li>ssh boot</li>
<li>tar -zxvf munge_build-*.tar.gz -C /rr/current</li>
</ul></p>

<h3>Configure Munge</h3>

<p>The following steps apply to each service node where
<ul>
<li>The <i>slurmd</i> or <i>slurmctld</i> daemon will run and/or</li>
<li>Users will be submitting jobs</li>
</ul></p>

<p><ol>
<li>On each login host create directory hierarchy:<br>
    <i>mkdir --mode=0711 -vp /var/lib/munge</i><br>
    <i>mkdir --mode=0700 -vp /var/log/munge</i><br>
    <i>mkdir --mode=0755 -vp /var/run/munge</i></li>
<li><i>module load slurm</i> as root, to get the correct SLURM paths</li>
<li><i>munged --key-file /opt/slurm/munge/etc/munge.key</i></li>
<li>Try a "<i>munge -n</i>" to see if munged accepts input</li>
</ol></p>

<p>When done, verify network connectivity by executing:
<ul>
<li>munge -n |ssh other-login-host /opt/slurm/munge/bin/unmunge</li>
</ul>


<p>If you decide to keep the installation, you may be interested in automating
the process using an <i>init.d</i> script distributed with the Munge as
<i>src/etc/munge.init</i>. This is installed by</p>
<ul>
<li>scp contribs/cray/etc_init_d_munge boot:/rr/current/software/munge</li>
<li>xtopview -c login</li>
<li>cp /software/munge /etc/init.d</li>
<li>chkconfig munge on</li>
</ul></p>

<h3>Build and Configure SLURM</h3>

<p>SLURM can be built and installed as on any other computer as described
<a href="quickstart_admin.html">Quick Start Administrator Guide</a>.
When building SLURM's slurm.conf configuration file, use the <i>NodeName</i>
parameter to specify all batch nodes to be scheduled.
If nodes are defined in ALPS, but not defined in the slurm.conf file, a
complete list of all batch nodes configured in ALPS will be logged by
the slurmctld daemon when it starts.
One would typically use this information to modify the slurm.conf file and
restart the surmctld daemon.
Note that the <i>NodeAddr</i> and <i>NodeHostName</i> fields should not be
configured, but will be set by SLURM using data from ALPS.
<i>NodeAddr</i> be set to the node's XYZ coordinate and be used by SLURM's
<i>smap</i> and <i>sview</i> commands.
<i>NodeHostName</i> will be set to the node's component label.
The format of the component label is "c#-#c#s#n#" where the "#" fields
represent in order: cabinet, row, cate, blade or slot, and node.
For example "c0-1c2s5n3" is cabinet 0, row 1, cage 3, slot 5 and node 3.</p>

<p>The <i>slurmd</i> daemons will not execute on the compute nodes, but will
execute on one or more front end nodes.
It is from here that batch scripts will execute <i>aprun</i> commands to
launch tasks.
This is specified in the <i>slurm.conf</i> file by using the
<i>FrontendName</i> and optionally the <i>FrontEndAddr</i> fields
as seen in the examples below.</p>

<p>You need to specify the appropriate resource selection plugin (the
<i>SelectType</i> option in SLURM's <i>slurm.conf</i> configuration file).
Configure <i>SelectType</i> to <i>select/cray</i> The <i>select/cray</i> 
plugin provides an interface to ALPS plus issues calls to the
<i>select/linear</i>, which selects resources for jobs using a best-fit
algorithm to allocate whole nodes to jobs (rather than individual sockets,
cores or threads).</p>

<p>Note that the system topology is based upon information gathered from
the ALPS database and is based upon the ALPS_NIDORDER configuration in
<i>/etc/sysconfig/alps</i>. Excerpts of a <i>slurm.conf</i> file for
use on a Cray systems follows:</p>

<pre>
#---------------------------------------------------------------------
# PLUGINS
#---------------------------------------------------------------------
# Network topology (handled internally by ALPS)
TopologyPlugin=topology/none

# Scheduling
SchedulerType=sched/backfill

# Node selection: use the special-purpose "select/cray" plugin.
# Internally this uses select/linar, i.e. nodes are always allocated
# in units of nodes (other allocation is currently not possible, since
# ALPS does not yet allow to run more than 1 executable on the same
# node, see aprun(1), section LIMITATIONS).
#
# Add CR_memory as parameter to support --mem/--mem-per-cpu.
SelectType=select/cray
SelectTypeParameters=CR_Memory

# How process IDs (parent/child) are tracked:
# proctrack/sgi_job is possible on Cray XT/XE since Cray provides
# libjob.so as part of its job/CSA package. However, the first job
# running on a host will always produce a 0 container ID, due to the
# way 64 bit job IDs are configured on Cray (the upper 32 bits are a
# host Id, which get truncated by slurm). The Linux fallback /proc
# can be used by enabling 'proctrac/linuxproc'.
ProctrackType=proctrack/sgi_job

#---------------------------------------------------------------------
# PATHS
#---------------------------------------------------------------------
SlurmdSpoolDir=/ufs/slurm/spool
StateSaveLocation=/ufs/slurm/spool/state

# main logfile
SlurmctldLogFile=/ufs/slurm/log/slurmctld.log
# slurmd logfiles (using %h for hostname)
SlurmdLogFile=/ufs/slurm/log/%h.log

# PIDs
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

#---------------------------------------------------------------------
# COMPUTE NODES
#---------------------------------------------------------------------
# Return DOWN nodes to service when e.g. slurmd has been unresponsive
ReturnToService=1

# Controls how a node's configuration specifications in slurm.conf are
# used.
# 0 - use hardware configuration (must agree with slurm.conf)
# 1 - use slurm.conf, nodes with fewer resources are marked DOWN
# 2 - use slurm.conf, but do not mark nodes down as in (1)
FastSchedule=2

# Per-node configuration for PALU AMD G34 dual-socket "Magny Cours"
# Compute Nodes. We deviate from slurm's idea of a physical socket
# here, since the Magny Cours hosts two NUMA nodes each, which is
# also visible in the ALPS inventory (4 Segments per node, each
# containing 6 'Processors'/Cores).
NodeName=DEFAULT Sockets=4 CoresPerSocket=6 ThreadsPerCore=1
NodeName=DEFAULT RealMemory=32000 State=UNKNOWN

# List the nodes of the compute partition below (service nodes are not
# allowed to appear)
NodeName=nid00[002-013,018-159,162-173,178-189]

# Frontend nodes: these should not be available to user logins, but
#                 have all filesystems mounted that are also 
#                 available on a login node (/scratch, /home, ...).
FrontendName=palu[7-9]

#---------------------------------------------------------------------
# ENFORCING LIMITS
#---------------------------------------------------------------------
# Enforce the use of associations: {associations, limits, wckeys}
AccountingStorageEnforce=limits

# Do not propagate any resource limits from the user's environment to
# the slurmd
PropagateResourceLimits=NONE

#---------------------------------------------------------------------
# Resource limits for memory allocation:
# * the Def/Max 'PerCPU' and 'PerNode' variants are mutually exclusive;
# * use the 'PerNode' variant for both default and maximum value, since
#   - slurm will automatically adjust this value depending on
#     --ntasks-per-node
#   - if using a higher per-cpu value than possible, salloc will just
#     block.
#--------------------------------------------------------------------
DefMemPerNode=32000
MaxMemPerNode=32000

#---------------------------------------------------------------------
# PARTITIONS
#---------------------------------------------------------------------
# defaults common to all partitions
PartitionName=DEFAULT Nodes=nid00[002-013,018-159,162-173,178-189]
PartitionName=DEFAULT MaxNodes=178
PartitionName=DEFAULT Shared=EXCLUSIVE State=UP DefaultTime=60

# "User Support" partition with a higher priority
PartitionName=usup Hidden=YES Priority=10 MaxTime=720 AllowGroups=staff

# normal partition available to all users
PartitionName=day Default=YES Priority=1 MaxTime=01:00:00
</pre>

<p>SLURM supports an optional <i>cray.conf</i> file containing Cray-specific
configuration parameters. <b>This file is NOT needed for production systems</b>,
but is provided for advanced configurations. If used, <i>cray.conf</i> must be
located in the same directory as the <i>slurm.conf</i> file.</p>

<dl>
<dt><b>AlpsDir</b>
<dd>Fully qualified pathname of the directory in which ALPS is installed.
The default value is <i>/usr</i>.
<dt><b>apbasil</b>
<dd>Fully qualified pathname to the apbasil command.
The default value is <i>/usr/bin/apbasil</i>.
<dt><b>apkill</b>
<dd>Fully qualified pathname to the apkill command.
The default value is <i>/usr/bin/apkill</i>.
<dt><b>SDBdb</b>
<dd>Name of the ALPS database.
The default value is <i>XTAdmin</i>.
<dt><b>SDBhost</b>
<dd>Hostname of the database server.
The default value is based upon the contents of the 'my.cnf' file used to
store default database access information and that defaults to user 'sdb'.
<dt><b>SDBpass</b>
<dd>Password used to access the ALPS database.
The default value is based upon the contents of the 'my.cnf' file used to
store default database access information and that defaults to user 'basic'.
<dt><b>SDBport</b>
<dd>Port used to access the ALPS database.
The default value is 0.
<dt><b>SDBuser</b>
<dd>Name of user used to access the ALPS database.
The default value is based upon the contents of the 'my.cnf' file used to
store default database access information and that defaults to user 'basic'.
</dl></p>

<pre>
# Example cray.conf file
apbasil=/opt/alps_simulator_40_r6768/apbasil.sh
SDBhost=localhost
SDBuser=alps_user
SDBdb=XT5istanbul
</pre>

<p>One additional configuration script can be used to insure that the slurmd
daemons execute with the highest resource limits possible, overriding default
limits on Suse systems. Depending upon what resource limits are propagated
from the user's environment, lower limits may apply to user jobs, but this
script will insure that higher limits are possible. Copy the file
<i>contribs/cray/etc_sysconfig_slurm</i> into <i>/etc/sysconfig/slurm</i>
for these limits to take effect. This script is executed from
<i>/etc/init.d/slurm</i>, which is typically executed to start the SLURM
daemons. An excerpt of <i>contribs/cray/etc_sysconfig_slurm</i>is shown
below.</p>

<pre>
#
# /etc/sysconfig/slurm for Cray XT/XE systems
#
# Cray is SuSe-based, which means that ulimits from
# /etc/security/limits.conf will get picked up any time SLURM is
# restarted e.g. via pdsh/ssh. Since SLURM respects configured limits,
# this can mean that for instance batch jobs get killed as a result
# of configuring CPU time limits. Set sane start limits here.
#
# Values were taken from pam-1.1.2 Debian package
ulimit -t unlimited	# max amount of CPU time in seconds
ulimit -d unlimited	# max size of a process's data segment in KB
</pre>

<p class="footer"><a href="#top">top</a></p>

<p style="text-align:center;">Last modified 16 April 2011</p></td>

<!--#include virtual="footer.txt"-->
