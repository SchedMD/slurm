<!--#include virtual="header.txt"-->

<h1>Accounting</h1>
<p>SLURM collects accounting information for every job and job step 
executed. 
Information is available about both currently executing jobs and 
jobs which have already terminated and can be viewed using the 
<b>sacct</b> command. 
Resource usage is reported for each task and this can be useful to 
detect load imbalance between the tasks. 
SLURM version 1.2 and earlier supported the storage of accounting 
records to a text file.
Beginning in SLURM version 1.3 accounting records can be written to 
a database. </p>

<p>There are three distinct plugin types associated with resource accounting.
The configuration parameters associated with these plugins include:
<ul>
<li><b>JobCompType</b> controls how job completion information is 
recorded. This can be used to record basic job information such
as job name, user name, allocated nodes, start time, completion 
time, exit status, etc. If the preservation of only basic job 
information is required, this plugin should satisfy your needs
with minimal overhead. You can store this information in a 
text file, <a href="http://www.mysql.com/">MySQL</a> or 
<a href="http://www.postgresql.org/">PostgreSQL</a> 
database optionally using either 
<a href="http://www.clusterresources.com/pages/products/gold-allocation-manager.php">Gold</a>
or SlurmDBD for added database security.</li>
<li><b>JobAcctGatherType</b> is operating system dependent and 
controls what mechanisms are used to collect accounting information.
Supported values are <i>jobacct_gather/aix</i>, <i>jobacct_gather/linux</i>
and <i>jobacct_gather/none</i> (no information collected).</li>
<li><b>AccountingStorageType</b> controls how detailed job and job 
step information is recorded. You can store this information in a 
text file, <a href="http://www.mysql.com/">MySQL</a> or 
<a href="http://www.postgresql.org/">PostgreSQL</a> 
database optionally using either 
<a href="http://www.clusterresources.com/pages/products/gold-allocation-manager.php">Gold</a>
or SlurmDBD for added security.</li>
</ul>

<p>Storing the information into text files is very simple. 
Just configure the appropriate plugin (e.g. 
<i>AccountingStorageType=accounting_storage/filetxt</i> and/or 
<i>JobCompType=jobcomp/filetxt</i>) and then specify the 
pathname of the file (e.g.
<i>AccountingStorageLoc=/var/log/slurm/accounting</i> and/or 
<i>JobCompLoc=/var/log/slurm/job_completions</i>).
Use the <i>logrotate</i> or similar tool to prevent the 
log files from getting too large.
Send a SIGHUP signal to the <i>slurmctld</i> deaemon 
after moving the files, but before compressing them so
that new log files will be created.</p>

<p>Storing the data directly into a database from SLURM may seem 
attractive, but that requires the availability of user name and 
password data not only for the SLURM control daemon (slurmctld), 
but also user commands which need to access the data (sacct and
sacctmgr). 
Making information available to all users makes database security 
more difficult to provide, sending the data through an intermediate
daemon can provide better security. 
Gold and SlurmDBD are two such services. 
Our initial implementation relied upon Gold, but we found its
performance to be inadequate for our needs and developed SlurmDBD.
SlurmDBD (SLURM Database Daemon) is written in C, multi-threaded, 
secure, and considerably faster than Gold.
The configuration required to use SlurmDBD will be described below.
Direct database or Gold use would be similar.</p>

<p>Note that SlurmDBD relies upon existing SLURM plugins
for authentication and database use, but the other SLURM 
commands and daemons are not required on the host where
SlurmDBD is installed. Install the <i>slurmdbd</i> and 
<i>slurm-plugins</i> RPMs on the computer when SlurmDBD
is to execute.</p>

<h2>Infrastructure</h2>

<p>If the SlurmDBD is executed on a different cluster than the 
one managed by SLURM, possibly to collect data from multiple 
clusters in a single location, there are some constraints on 
the user space.
The user ID associated with <i>SlurmUser</i> must be uniform 
across all clusters. 
Accounting is maintained by user name (not user ID), but a
given user name should refer to the same person across all 
of the computers.</p>

<p>The best way to insure security of the data is by authenticating 
communications to the SlurmDBD and we recommend 
<a href="http://home.gna.org/munge/">Munge</a> for that purpose.
Munge was designed to support authentication within a cluster.
If you have one cluster managed by SLURM and execute the SlurmDBD 
on that one cluster, the normal Munge configuration will suffice.
Otherwise Munge should then be installed on all nodes of all 
SLURM managed clusters plus the machine where SlurmDBD executes.
You then have a choice of either having a single Munge key for 
all of these computers or maintaining a unique key for each of the 
clusters plus a second key for communications between the clusters
for better security.
Munge enhancements are planned to support two keys within a single 
configuration file, but presently two different daemons must be 
started with different configuration files to support two different 
keys. 
If a Munge separate daemon configured to provide enterprise-wide 
authentication, it will have a unique named pipe configured for 
communications. 
The pathname of this pipe will be needed in the SLURM and SlurmDBD
configuration files (slurm.conf and slurmdbd.conf respectively, 
more details are provided below).</p>

<h2>SLURM Configuration</h2>

<p>Several SLURM configuration parameters must be set to support
archiving information in SlurmDBD. SlurmDBD has a separate configuration
file which is documented in a separate section.
Note that you can write accounting information to SlurmDBD
while job completion records are written to a text file or 
not maintained at all. 
If you don't set the configuration parameters that begin 
with "JobComp" then job completion records will not be recorded.</p>

<ul>
<li><b>AccountingStorageEnforce</b>:
If you want to prevent users from running jobs if their <i>association</i>
(a combination of cluster name, partition name, user name, and account name)
is not in the database, then set this to "1".
Otherwise jobs will be executed based upon policies configured in 
SLURM on each cluster.</li>

<li><b>AccountingStorageHost</b>:
The name or address of the host where SlurmDBD executes.</li>

<li><b>AccountingStoragePass</b>:
If using SlurmDBD with a second Munge daemon, store the pathname of 
the named socket used by Munge to provide enterprise-wide.
Otherwise the default Munge daemon will be used.</li>

<li><b>AccountingStoragePort</b>:
The network port that SlurmDBD accepts communication on.</li>

<li><b>AccountingStorageType</b>:
Set to "accounting_storage/slurmdbd".</li>

<li><b>ClusterName</b>:
Set to a unique name for each Slurm-managed cluster so that 
accounting records from each can be identified.</li>

<li><b>JobCompHost</b>:
The name or address of the host where SlurmDBD executes.</li>

<li><b>JobCompPass</b>:
If using SlurmDBD with a second Munge daemon, store the pathname of 
the named socket used by Munge to provide enterprise-wide.
Otherwise the default Munge daemon will be used.</li>

<li><b>JobCompPort</b>:
The network port that SlurmDBD accepts communication on.</li>

<li><b>JobCompType</b>:
Set to "jobcomp/slurmdbd".</li>
</ul>

<h2>SlurmDBD Configuration</h2>

<p>SlurmDBD requires its own configuration file called "slurmdbd.conf". 
This file should be only on the computer where SlurmDBD executes and 
should only be readable by the user which executes SlurmDBD (e.g. "slurm").
This file should be protected from unauthorized access since it
contains a database login name and password.
See "man slurmdbd.conf" for a more complete description of the 
configuration parameters. 
Some of the more important parameters include:</p>

<ul>
<li><b>AuthInfo</b>:
If using SlurmDBD with a second Munge daemon, store the pathname of 
the named socket used by Munge to provide enterprise-wide.
Otherwise the default Munge daemon will be used.</li>

<li><b>AuthType</b>:
Define the authentication method for communications between SLURM 
components. A value of "auth/munge" is recommended.</li>

<li><b>DbdHost</b>:
The name of the machine where the Slurm Database Daemon is executed. 
This should be a node name without the full domain name (e.g. "lx0001"). 
This value must be specified.</li>

<li><b>DbdPort</b>:
The port number that the Slurm Database Daemon (slurmdbd) listens 
to for work. The default value is SLURMDBD_PORT as established at system 
build time. If none is explicitly specified, it will be set to 6819.
This value must be equal to the <i>SlurmDbdPort</i> parameter in the
slurm.conf file.</li>

<li><b>LogFile</b>:
Fully qualified pathname of a file into which the Slurm Database Daemon's 
logs are written.
The default value is none (performs logging via syslog).</li>

<li><b>PluginDir</b>:
Identifies the places in which to look for SLURM plugins. 
This is a colon-separated list of directories, like the PATH 
environment variable. 
The default value is "/usr/local/lib/slurm".</li>

<li><b>SlurmUser</b>:
The name of the user that the <i>slurmctld</i> daemon executes as. 
This user must exist on the machine executing the Slurm Database Daemon
and have the same user ID as the hosts on which <i>slurmctld</i> execute.
For security purposes, a user other than "root" is recommended.
The default value is "root". </li>

<li><b>StorageHost</b>:
Define the name of the host the database is running where we are going
to store the data.
Ideally this should be the host on which slurmdbd executes.</li>

<li><b>StorageLoc</b>:
Specifies the location of the database where accounting 
records are written.</li>

<li><b>StoragePass</b>:
Define the password used to gain access to the database to store 
the job accounting data.</li>

<li><b>StoragePort</b>:
Define the password used to gain access to the database to store 
the job accounting data.</li>

<li><b>StorageType</b>:
Define the accounting storage mechanism type.
Acceptable values at present include 
"accounting_storage/gold", "accounting_storage/mysql", and
"accounting_storage/pgsql".
The value "accounting_storage/gold" indicates that account records
will be written to Gold, which maintains its own database.
Use of Gold is not recommended due to reduced performance without 
providing any additional security.
The value "accounting_storage/mysql" indicates that accounting records
should be written to a MySQL database specified by the 
\fStorageLoc\fR parameter.
The value "accounting_storage/pgsql" indicates that accounting records
should be written to a PostgreSQL database specified by the 
\fBStorageLoc\fR parameter.
This value must be specified.</li>

<li><b>StorageUser</b>:
Define the name of the user we are going to connect to the database
with to store the job accounting data.</li>
</ul>

<h2>Database Configuration</h2>

<p>Accounting records are maintained based upon what we refer 
to as an <i>Association</i>, which consists of four elements:
cluster name, partition name, user name, and account name. 
Use the <i>sacctmgr</i> command to create and manage these records.
You will want to define the names of clusters being managed
by Slurm, the users with accounts on these computers, plus
the user's default and valid account names. Partition names
will be uploaded from Slurm on the cluster, but can be 
explicitly defined if so desired.</p>

<h2>Node State Information</h2>

<p>Node state information is also recorded in the database. 
Whenever a node goes DOWN or becomes DRAINED that event is 
logged along with the node's <i>Reason</i> field. 
This can be used to generate various reports.</p>

<h2>Tools</h2>

<p>There are two tools available to work with accounting data,
sacct and sacctmgr. Both of these tools will get or set data
through the SlurmDBD daemon. 
Sacct is used to generate accounting report for both running and 
completed jobs.
Sacctmgr is used to manage associations in the database: 
add or remove clusters, add or remove users, etc.
See the man pages for each command for more information.</p>

<p>Web interfaces with graphical output is currently under
development and should be available in the summer of 2008.
A tool to report node state information is also under development.</p>


<p style="text-align:center;">Last modified 19 March 2008</p>

<!--#include virtual="footer.txt"-->
