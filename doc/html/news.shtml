<!--#include virtual="header.txt"-->

<h1>What's New</h1>

<h2>Index</h2>
<ul>
<li><a href="#26">Slurm Version 2.6, June 2013</a></li>
<li><a href="#1403">Slurm Version 14.03, March 2014</a></li>
<li><a href="#1409">Slurm Version 14.09 and beyond</a></li>
</ul>

<h2><a name="26">Major Updates in Slurm Version 2.6</a></h2>
<p>SLURM Version 2.6 was released in July 2013.
Major enhancements include:
<ul>
<li>Support for job arrays, which increases performance and ease of use
    for sets of similar jobs.</li>
<li><a href="hdf5_profile_user_guide.html">Job profiling</a> capability added
    to record a wide variety of job characteristics for each task on a user
    configurable periodic basis. Data currently available includes CPU use,
    memory use, energy use, Infiniband network use, Lustre file system use,
    etc.</li>
<li>Support for MPICH2 using PMI2 communications interface with much greater
    scalability.</li>
<li>Prolog and epilog support for advanced reservations.</li>
<li>Much faster throughput for job step execution with --exclusive option. The
    srun process is notified when resources become available rather than
    periodic polling.</li>
<li>Support improved for Intel MIC (Many Integrated Core) processor.</li>
<li>Advanced reservations with hostname and core counts now supports asymmetric
    reservations (e.g. specific different core count for each node).</li>
<li><a href="ext_sensorsplugins.html">External sensor plugin infrastructure</a>
    added to record power consumption, temperature, etc.</li>
<li>Improved performance for high-throughput computing.</li>
<li><a href="http://en.wikipedia.org/wiki/MapReduce">MapReduce+</a> support
    (launches ~1000x faster, runs ~10x faster).</li>
<li>Added "MaxCPUsPerNode" partition configuration parameter. This can be
    especially useful to schedule GPUs. For example a node can be associated
    with two Slurm partitions (e.g. "cpu" and "gpu") and the partition/queue
    "cpu" could be limited to only a subset of the node's CPUs, insuring that
    one or more CPUs would be available to jobs in the "gpu" partition/queue.</li>
</ul>
</p>

<h2><a name="1403">Major Updates in Slurm Version 14.03</a></h2>
<p>NOTICE: Starting with 14.03 we will be numbering Slurm versions
with a YEAR.MONTH format.<br>
SLURM Version 14.03 release is planned in March 2014.
Major enhancements include:
<ul>
<li>Integration with
    <a href="http://en.wikipedia.org/wiki/FlexNet_Publisher">FLEXlm (Flexnet Publisher)</a>
    license management.</li>
<li>Layouts framework, which will be the basis for further developments toward
    optimizing scheduling with respect to additional parameters such as temperature
    and power consumption.</li>
<li>Energy consumption added as a factor in fair-share scheduling.</li>
<li>Energy aware scheduling added with respect to power caps.</li>
<li>Improved user support for fault-tolerance (e.g. "hot spare" resources).</li>
<li>New partition configuration parameters: AllowAccounts, AllowQOS,
    DenyAccounts and DenyQOS.</li>
<li>Scalability improvements for MPI initialization including communication of
    the compute node network interface details.</li>
<li>Defer sending SIGKILL signal to processes while a core dump is in progress.</li>
<li>Other important enhancements that can not be made public at this time...</li>
</ul>
</p>

<h2><a name="1409">Major Updates in Slurm Version 14.09 and beyond</a></h2>
<p> Detailed plans for release dates and contents of additional SLURM releases
have not been finalized. Anyone desiring to perform SLURM development should
notify <a href="mailto:slurm-dev@schedmd.com">slurm-dev@schedmd.com</a>
to coordinate activities. Future development plans includes:
<ul>
<li>Distributed architecture to support the management of resources with Intel
    MIC processors.</li>
<li>Support of I/O as a new resources, including proxy I/O nodes with data
    locality.</li>
<li>Improved scheduling support for job dependencies (e.g. pre-processing,
    post-processing, co-processing on I/O nodes, etc.) to optimize overall
    system utilization.</li>
<li>IP communications over InfiniBand network for improved performance.</li>
<li>Support for heterogeneous GPU environments (i.e. user specification of
    desired GPU types).</li>
<li>Fault-tolerance and jobs dynamic adaptation through communication protocol
    between Slurm , MPI libraries and the application.</li>
<li>Improved support for high-throughput computing (e.g. multiple slurmctld
    daemons on a single cluster).</li>
<li>Scheduling fully optimized for energy efficiency.</li>
<li>Numerous enhancements to advanced resource reservations (e.g. start or
    end the reservation early depending upon the workload).</li>
<li>Add Kerberos credential support including credential forwarding
    and refresh.</li>
<li>Improved support for provisioning and virtualization.</li> 
<li>Provide a web-based SLURM administration tool.</li>
<li>Finer-grained BlueGene resouce management (partitions/queues and advanced
    reservations containing less than a whole midplane).</li>
</ul>

<p style="text-align:center;">Last modified 05 December 2013</p>

<!--#include virtual="footer.txt"-->
