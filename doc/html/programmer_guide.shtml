<!--#include virtual="header.txt"-->

<h1><a name="top">SLURM Programmer's Guide</a></h1>

<h2>Overview</h2>

<p>Simple Linux Utility for Resource Management (SLURM) is an open source, fault-tolerant, 
and highly scalable cluster management and job scheduling system for large and 
small Linux clusters. Components include machine status, partition management, 
job management, scheduling, and stream copy modules. SLURM requires no kernel 
modifications for it operation and is relatively self-contained. 
<p>There is an overview of the components and their interactions available in 
a separate document, <a href="slurm_design.pdf"> SLURM: Simple Linux Utility for 
Resource Management</a> [PDF]. 

<p>SLURM is written in the C language and uses a GNU <b>autoconf</b> configuration 
engine. While initially written for Linux, other UNIX-like operating systems should 
be easy porting targets. Code should adhere to the <a href="coding_style.pdf"> 
Linux kernel coding style</a>. <i>(Some components of SLURM have been taken from 
various sources. Some of these components are written in C++ or do not conform 
to the Linux kernel coding style. However, new code written for SLURM should 
follow these standards.)</i>
 
<p>Many of these modules have been built and tested on a variety of Unix computers 
including Red Hat Linux, IBM's AIX, Sun's Solaris, and Compaq's Tru-64. The only 
module at this time that is operating system dependent is <span class="commandline">src/slurmd/read_proc.c</span>. 
We will be porting and testing on additional platforms in future releases. 

<h2>Plugins</h2>

<p>To make the use of different infrastructures possible, SLURM uses a general 
purpose plugin mechanism. A SLURM plugin is a dynamically linked code object that 
is loaded explicitly at run time by the SLURM libraries. It provides a customized 
implementation of a well-defined API connected to tasks such as authentication, 
interconnect fabric, task scheduling, etc. A set of functions is defined for use 
by all of the different infrastructures of a particular variety. When a SLURM 
daemon is initiated, it reads the configuration file to determine which of the 
available plugins should be used. A <a href="plugins.html">plugin developer's 
guide</a> is available with general information about plugins. Most plugin 
types also have their own documenation available, such as 
<a href="authplugins.html">SLURM Authentication Plugin API</a> and
<a href="jobcompplugins.html">SLURM Job Completion Logging API</a>.</p>

<p class="footer"><a href="#top">top</a></p>

<h2>Directory Structure</h2>

<p>The contents of the SLURM directory structure will be described below in increasing 
detail as the structure is descended. The top level directory contains the scripts 
and tools required to build the entire SLURM system. It also contains a variety 
of subdirectories for each type of file.</p>
<p>General build tools/files include: <b>acinclude.m4</b>, <b>autogen.sh</b>, 
<b>configure.ac</b>, <b>Makefile.am</b>, <b>Make-rpm.mk</b>, <b>META</b>, <b>README</b>, 
<b>slurm.spec.in</b>, and the contents of the <b>auxdir</b> directory. <span class="commandline">autoconf</span> 
and <span class="commandline">make</span> commands are used to build and install 
SLURM in an automated fashion. NOTE: <span class="commandline">autoconf</span> 
version 2.52 or higher is required to build SLURM. Execute 
<span class="commandline">autoconf -V</span> to check your version number. 
The build process is described in the README file. 

<p>Copyright and disclaimer information are in the files COPYING and DISCLAIMER. 
All of the top-level subdirectories are described below.</p>
<p style="margin-left:.2in"><b>auxdir</b>&#151;Used for building SLURM.<br>
<b>doc</b>&#151;Documentation including man pages. <br>
<b>etc</b>&#151;Sample configuration files.<br>
<b>slurm</b>&#151;Header files for API use. These files must be installed. Placing 
these header files in this location makes for better code portability.<br>
<b>src</b>&#151;Contains all source code and header files not in the "slurm" subdirectory 
described above.<br>
<b>testsuite</b>&#151;DejaGnu is used as a testing framework and all of its files 
are here.</p>

<p class="footer"><a href="#top">top</a></p>

<h2>Documentation</h2>
<p>All of the documentation is in the subdirectory <b>doc</b>. Man pages for the 
APIs, configuration file, commands, and daemons are in <b>doc/man</b>. Various 
documents suitable for public consumption are in <b>doc/html</b>. Overall SLURM 
design documents including various figures are in <b>doc/pubdesign</b>. Various 
design documents (many of which are dated) can be found in <b>doc/slides</b> and 
<b>doc/txt</b>. A survey of available resource managers as of 2001 is in <b>doc/survey</b>. 
<h2>Source Code</h2>

<p>Functions are divided into several categories, each in its own subdirectory. 
The details of each directory's contents are proved below. The directories are 
as follows: </p>

<p style="margin-left:.2in"><b>api</b>&#151;Application Program Interfaces into 
the SLURM code. Used to send and get SLURM information from the central manager. 
These are the functions user applications might utilize.<br>
<b>api</b>&#151;Application Programming Interfaces for SLURM commands and 
user application use.<br>
<b>common</b>&#151;General purpose functions for widespread use throughout 
SLURM.<br>
<b>plugins</b>&#151;Plugin functions for various infrastructure. A separate 
subdirectory is used for each plugin class:<br> 
<ul>
<li><span class="commandline">auth</span> for user authentication,<br> 
<li><span class="commandline">checkpoint</span> for system-initiated checkpoint 
and restart of user jobs,<br>
<li><span class="commandline">jobacct</span> for job accounting,<br>
<li><span class="commandline">jobcomp</span> for job completion logging,<br>
<li><span class="commandline">mpi</span> for MPI support,<br>
<li><span class="commandline">proctrack</span> for process tracking,<br>
<li><span class="commandline">sched</span> for job scheduler,<br> 
<li><span class="commandline">select</span> for a job's node selection,<br>
<li><span class="commandline">switch</span> for switch (interconnect) specific 
functions,<br>
<li><span class="commandline">task</span> for task affinity to processors.<br>
</ul>
<b>sbcast</b>&#151;User command to broadcast a file to all nodes associated 
with an existing SLURM job.<br>
<b>scancel</b>&#151;User command to cancel (or signal) a job or job step.<br>
<b>scontrol</b>&#151;Administrator tool to manage SLURM.<br>
<b>sinfo</b>&#151;User command to get information on SLURM nodes and partitions.<br>
<b>slurmctld</b>&#151;SLURM central manager daemon code.<br>
<b>slurmd</b>&#151;SLURM daemon code to manage the compute server nodes including 
the execution of user applications.<br>
<b>smap</b>&#151;User command to view layout of nodes, partitions, and jobs.
This is particularly valuable on systems like Blue Gene, which has a three
dimension torus topography.<br>
<b>squeue</b>&#151;User command to get information on SLURM jobs and job steps.<br>
<b>srun</b>&#151;User command to submit a job, get an allocation, and/or 
initiation a parallel job step.</p>

<p class="footer"><a href="#top">top</a></p>

<h2>Configuration</h2>
<p>Configuration files are included in the <b>etc</b> subdirectory. <b>slurm.conf.example</b> 
includes a description of all configuration options and default settings. See 
<b>doc/man/man5/slurm.conf.5</b> for more details.
<span class="commandline">init.d.slurm</span> 
is a script that determines which SLURM daemon(s) should execute on any node based 
upon the configuration file contents. It will also manage these daemons: starting, 
signalling, restarting, and stopping them.</p> 

<h2>Test Suite</h2>
<p>The <b>testsuite</b> files use a DejaGnu framework for testing. These tests 
are very limited in scope. We also have a set of Expect SLURM tests available 
as a separate distribution. These tests are executed after SLURM has been installed 
and the daemons initiated. About 110 test scripts exercise all SLURM commands 
and options including stress tests. Get these test from
<a href="ftp://ftp.llnl.gov/pub/linux/slurm-qa">ftp://ftp.llnl.gov/pub/linux/slurm-qa</a></p>

<h2>Adding Files and Directories</h2>
<p>If you are adding files and directories to SLURM, it will be necessary to
re-build configuration files before executing the <b>configure</b> command.
Update <b>Makefile.am</b> files as needed then execute 
<b>autogen.sh</b> before executing <b>configure</b>.

<h2>Tricks of the Trade</h2>
<h3>HAVE_FRONT_END</h3>
<p>You can make a single node appear to SLURM as a Linux cluster by manually 
defining <b>HAVE_FRONT_END</b> to have a non-zero value in the file <b>config.h</b>.
All (fake) nodes should be defined in the <b>slurm.conf</b> file.
These nodes should be configured with a single <b>NodeAddr</b> value
indicating the node on which single <span class="commandline">slurmd</span> daemon 
executes.  Initiate one <span class="commandline">slurmd</span> and one 
<span class="commandline">slurmctld</span> daemon. Do not initiate too many 
simultaneous job steps to avoid overloading the 
<span class="commandline">slurmd</span> daemon executing them all.</p>

<h3>Multiple slurmd support</h3>
<p>It is possible to run mutiple slurmd daemons on a single node, each using
a different port number and NodeName alias.  This is very useful for testing
networking and protocol changes, or anytime you want to simulate a larger
cluster than you really have.  The author uses this on his desktop to simulate
multiple nodes.  However, multiple slurmd mode should not be used in
production, because not all slurm functions are working under this mode (e.g.
many switch plugins will not work, srun reattachs won't work, etc.).</p>

<p>Multiple support is enabled at configure-time with the
"--enable-multiple-slurmd" parameter.  This enables a new parameter in the
slurm.conf file on the NodeName line, "Port=<port number>", and adds two new command
line parameters to slurmd, "-N" and "-P".</p>

<p>Each slurmd needs to have its own NodeName, and its own TCP port number. Here is
an example of the NodeName lines for running three slurmd daemons on each
of ten nodes:</p>

<pre>
NodeName=foo[1-10] NodeAddr=host[1-10]  Port=17001
NodeName=foo[11-20] NodeAddr=host[1-10] Port=17002
NodeName=foo[21-30] NodeAddr=host[1-10] Port=17003
</pre>

<p>
It is then up to you to start the slurmd daemons with the proper NodeName and
Port values.  For example, to start the slurmd daemons for host1 from the
above slurm.conf example:</p>

<pre>
host1> slurmd -N foo1 -P 17001
host1> slurmd -N foo11 -P 17002
host1> slurmd -N foo21 -P 17003
</pre>

<p class="footer"><a href="#top">top</a></p>

<p style="text-align:center;">Last modified 15 March 2006</p>

<!--#include virtual="footer.txt"-->
