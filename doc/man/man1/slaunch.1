.\" $Id$
.TH "slaunch" "1" "SLURM 1.2" "July 2006" "SLURM Commands"
.SH "NAME"
.LP 
slaunch \- Launch a parallel application under a SLURM job allocation.
.SH "SYNOPSIS"
.LP 
slaunch [\fIoptions\fP] <\fIcommand\fP> [\fIcommand args\fR]
.SH "DESCRIPTION"
.LP 
slaunch launches a parallel application (a \fBjob step\fR in SLURM parlance)
on the nodes, or subset of nodes, in a \fBjob allocation\fR.   A valid job
allocation is a prerequisite of running slaunch.  The ID of the job allocation
may be passed to slaunch through either the \fB\-\-jobid\fR command line
parameter or the \fBSLAUNCH_JOBID\fR environment variable.  The \fBsalloc\fR
and \fBsbatch\fR commands may be used to request a job allocation, and each
of those commands automatically set the \fBSLURM_JOB_ID\fR environment variable,
which is also understood by slaunch.  Users should not set SLURM_JOB_ID on their
own; use SLAUNCH_JOBID instead.
.SH "OPTIONS"
.LP 
.TP 
\fB\-\-jobid\fR <\fIJOBID\fP>
The job allocation under which the parallel application should be launched.  If slaunch is running under salloc or a batch script, slaunch can automatically determint the jobid from the SLURM_JOB_ID environment variable.  Otherwise, you will need to tell slaunch which job allocation to use.
.TP 
\fB\-n\fR, \fB\-\-tasks\fR[=]<\fInumber\fR>
Specify the number of processes to launch.  The default is one process per node.
.TP 
\fB\-N\fR, \fB\-\-nodes\fR[=]<\fInumber\fR>
Specify the number of nodes to be used by this job step.  By default,
slaunch will use all of the nodes in the specified job allocation.
.TP 
\fB\-r\fR, \fB\-\-relative\fR[=]<\fInumber\fR>
Specify the first node in the allocation on which this job step will be launched.  Counting starts at zero, thus the first node in the job allocation is node 0.  The option to \-\-relative may also be a negative number.  \-1 is the last node in the allocation, \-2 is the next to last node, etc.  By default, the controller will select the starting node (assuming that there are no other nodelist or task layout options that specify specific nodes).

.TP 
\fB\-c\fR, \fB\-\-cpus\-per\-task\fR[=]<\fIncpus\fR>
Specify that each task requires \fIncpus\fR number of CPUs.  Useful for applications in which each task will launch multiple threads and can therefore benefit from there being free processors on the node.

.TP
\fB\-J\fR, \fB\-\-name\fR[=]<\fIname\fR>
Set the name of the job step.  By default, the job step's name will be the
name of the executable which slaunch is launching.

.TP
\fB\-K\fR, \fB\-\-kill\-on\-bad\-exit\fR
Terminate the job step if any task exits with a non-zero exit code.  By default
slaunch will not terminate a job step because of a task with a non-zero exit
code.

.TP
\fB\-l\fR, \fB\-\-label\fR
Prepend each line of task standard output or standard error with the task
number of its origin.

.TP
\fB\-u\fR, \fB\-\-unbuffered\fR
Do not line buffer standard output or standard error from remote tasks.
This option cannot be used with \-\-label.

.TP
\fB\-C\fR, \fB\-\-overcommit\fR
Permit the allocation of more tasks to a node than there are available processors.
Normally SLURM will only allow up to N tasks on a node with N processors, but
this option will allow more than N tasks to be assigned to a node.

.TP
\fB\-q\fR, \fB\-\-quiet\fR
Suppress informational messages from slaunch. Errors will still be displayed.

.TP
\fB\-v\fR, \fB\-\-verbose\fR
Increase the verbosity of slaunch's informational messages.  Multiple \-v's
will further increase slaunch's verbosity.

.TP
\fB\-d\fR, \fB\-\-slurmd-debug\fR[=]<\fIlevel\fR>
Specify a debug level for slurmd(8). \fIlevel\fR may be an integer value
between 0 [quiet, only errors are displayed] and 4 [verbose operation]. 
The slurmd debug information is copied onto the stderr of
the job.  By default only errors are displayed. 

.TP
\fB\-W\fR, \fB\-\-wait\fR[=]<\fIseconds\fR>
slaunch will wait the specified number of seconds after the first tasks exits
before killing all tasks in the job step.  If the value is 0, slaunch will
wait indefinitely for all tasks to exit.  The default value is give by the
WaitTime parameter in the slurm configuration file (see \fBslurm.conf(5)\fR).

The \-\-wait option can be used to insure that a job step terminates in a timely
fashion in the event that one or more tasks terminate prematurely.

.TP
\fB\-m\fR, \fB\-\-distribution\fR[=]<\fI(cyclic|block|arbitrary)\fR>
Select a task distribution pattern.  (The default is cyclic.)
.RS
.TP
.B cyclic
The cyclic method distributes processes in a round-robin fashion across
the allocated nodes. That is, task zero will be allocated to the first
node, task one to the second, and so on.
.TP
.B block
The block method of distribution will allocate tasks in-order to
the cpus on a node. If the number of tasks exceeds the number of 
cpus on all of the nodes in the allocation then all nodes will be 
utilized. For example, consider an allocation of three nodes each with 
two cpus. A four\-task block distribution request will distribute 
those tasks to the nodes with tasks zero and one on the first 
node, task two on the second node, and task three on the third node.
.TP
.B arbitrary
The arbitrary method of distribution allows the user to manually specify any
arbitrary layout of tasks on nodes.  Normally the arbitrary task distribution
method will be invoked implicitly by using one of the options
\-\-task\-layout\-byid, \-\-task\-layout\-byname, or \-\-task\-layout\-file.
.RE

.TP 
\fB\-w\fR, \fB\-\-nodelist\-byname\fR[=]<\fInode name list\fR>
Request a specific list of node names.  The list may be specified as a comma\-separated list of node names, or a range of node names (e.g. mynode[1\-5,7,...]).  Duplicate node names are not permitted in the list.
The order of the node names in the list is not important; the node names
will be sorted my SLURM.
.TP 
\fB\-L\fR, \fB\-\-nodelist\-byid\fR[=]<\fInode index list\fR>
Request a specific set of nodes in a job alloction on which to run the tasks of the job step.  The list may be specified as a comma\-separated list relative node indices in the job allocation (e.g., "0,2\-5,\-2,8").  Duplicate indices are permitted, but are ignored.  The order of the node indices in the list is not important; the node indices will be sorted my SLURM.

.TP 
\fB\-T\fR, \fB\-\-task\-layout\-byid\fR[=]<\fInode index list\fR>
Request a specific task layout using node indices within the job allocation.  The node index list can contain duplicate indices, and the indices may appear in any order.  The order of indices in the nodelist IS significant.  Each node index in the list represents one task, with the Nth node index in the list designating on which node the Nth task should be launched.

For example, given an allocation of nodes "linux[0\-15]" and a node index list "4,\-1,1\-3" task 0 will run on "linux4", task 1 will run on "linux15", task 2 on "linux1", task 3 on "linux2", and task 4 on "linux3".

NOTE: This option implicitly sets the task distribution method to "arbitrary".  Some network switch layers do not permit arbitrary task layout.

.TP 
\fB\-Y\fR, \fB\-\-task\-layout\-byname\fR[=]<\fInode name list\fR>
Request a specific task layout.  The nodelist can contain duplicate node
names, and node names may appear in any order.  The order of node names in
the nodelist IS significant.  Each node name in the nodes list represents
one task, with the Nth node name in the nodelist designating on which node
the Nth task should be launched.  For example, a nodelist of mynode[4,3,1\-2,4]
means that tasks 0 and 4 will run on mynode4, task 1 will run on mynode3,
task 2 will run on mynode1, and task 3 will run on mynode2.

NOTE: This option implicitly sets the task distribution method to "arbitrary".
Some network switch layers do not permit arbitrary task layout.

.TP 
\fB\-F\fR, \fB\-\-task\-layout\-file\fR[=]<\fIfilename\fR>
Request a specific task layout.  This options much like the \-\-task\-layout
option, except that instead of a nodelist you supply the name of a file.
The file contains a nodelist that may span multiple lines of the file.

NOTE: This option implicitly sets the task distribution method to "arbitrary".
Some network switch layers do not permit arbitrary task layout.

.TP 
\fB\-i\fR, \fB\-\-slaunch\-input\fR[=]<\fIfilename pattern\fR>
.PD 0 
.TP
\fB\-o\fR, \fB\-\-slaunch\-output\fR[=]<\fIfilename pattern\fR>
.PD 0
.TP 
\fB\-e\fR, \fB\-\-slaunch\-error\fR[=]<\fIfilename pattern\fR>
.PD
Change slaunch's standard input, standard output, or standard error
to be a file of name "filename pattern".  These options are similar to using
shell IO redirection capabilities, but with the additional ability to replace
certain symbols in the filename with useful SLURM information.  Symbols are
listed below.

By default, slaunch broadcasts its standard input over the network to the
standard input of all tasks.  Likewise, standard output and standard error
from all tasks are collected over the network by slaunch and printed on
its standard output or standard error, respectively.  If you want to see
traffic from fewer tasks, see the \-\-slaunch\-[input|output|error]\-filter
options.

Supported replacement symbols are:
.PD 0
.RS 10
.TP 
\fB%J\fR
Job allocation number and job step number in the form "jobid.stepid".  For instance, "128.0".
.PD 0
.TP 
\fB%j\fR
Job allocation number.
.PD 0
.TP 
\fB%s\fR
Job step number.
.RS -10

.TP 
\fB\-\-slaunch\-input\-filter\fR[=]<\fItask number\fR>
.PD 0
.TP
\fB\-\-slaunch\-output\-filter\fR[=]<\fItask number\fR>
.PD 0
.TP 
\fB\-\-slaunch\-error\-filter\fR[=]<\fItask number\fR>
.PD
Only transmit standard input to a single task, or print the standard output
or standard error from a single task.  These options perform the filtering
locally in slaunch.  All tasks are still capable of sending or receiving
standard IO over the network, so the "sattach" command can still access the
standard IO streams of the other tasks.  (NOTE: for -output and -error,
the streams from all tasks WILL be transmitted to slaunch, but it will only
print the streams for the selected task.  If your tasks print a great deal of
data to standard output or error, this can be performance limiting.)

.TP
\fB\-I\fR, \fB\-\-task\-input\fR[=]<\fIfilename pattern\fR>
.PD 0
.TP
\fB\-O\fR, \fB\-\-task\-output\fR[=]<\fIfilename pattern\fR>
.PD 0
.TP
\fB\-E\fR, \fB\-\-task\-error\fR[=]<\fIfilename pattern\fR>
.PD
Instruct SLURM to connect each task's standard input, standard output,
or standard error directly to the file name specified
in the "\fIfilename pattern\fR".

By default, the standard IO streams of all tasks are received and transmitted
over the network to commands like slaunch and sattach.  These options disable
the networked standard IO streams and instead connect the standard IO streams
of the tasks directly to files on the local node of each task (although the file
may, of course, be located on a networked filesystem).

Whether or not the tasks share a file depends on whether or not the file lives
on a local filesystem or a shared network filesytem, and on whether or not
the filename pattern expands to the same file name for each task.

The filename pattern may
contain one or more replacement symbols, which are a percent sign "%" followed 
by a letter (e.g. %t).

Supported replacement symbols are:
.PD 0
.RS 10
.TP 
\fB%J\fR
Job allocation number and job step number in the form "jobid.stepid".  For instance, "128.0".
.PD 0
.TP 
\fB%j\fR
Job allocation number.
.PD 0
.TP 
\fB%s\fR
Job step number.
.PD 0
.TP 
\fB%N\fR
Node name. (Will result in a separate file per node.)
.PD 0
.TP 
\fB%n\fR
Relative node index number within the job step.  All nodes used by the job step will be number sequentially starting at zero.  (Will result in a separate file per node.)
.PD 0
.TP 
\fB%t\fR
Task rank number.  (Will result in a separate file per task.)
.RS -10

.TP 
\fB\-D\fR, \fB\-\-workdir\fR[=]<\fIdirectory\fR>
Set the working directory of the tasks to \fIdirectory\fR before execution.
The default task working directory is slaunch's working directory.

.TP 
\fB\-\-mpi\fR[=]<\fImpi_type\fR>
Identify the type of MPI to be used.

.TP
\fB\-\-uid\fR[=]<\fIuser\fR>
Attempt to submit and/or run a job as \fIuser\fR instead of the
invoking user id. The invoking user's credentials will be used
to check access permissions for the target partition. User root
may use this option to run jobs as a normal user in a RootOnly
partition for example. If run as root, \fBslaunch\fR will drop
its permissions to the uid specified after node allocation is
successful. \fIuser\fR may be the user name or numerical user ID.

.TP
\fB\-\-gid\fR[=]<\fIgroup\fR>
If \fBslaunch\fR is run as root, and the \fB\-\-gid\fR option is used, 
submit the job with \fIgroup\fR's group access permissions.  \fIgroup\fR 
may be the group name or the numerical group ID.

.TP
\fB\-\-core\fR[=]<\fItype\fR>
Adjust corefile format for parallel job. If possible, slaunch will set
up the environment for the job such that a corefile format other than
full core dumps is enabled. If run with type = "list", slaunch will
print a list of supported corefile format types to stdout and exit.

.TP
\fB\-\-propagate\fR[=\fIrlimits\fR]
Allows users to specify which of the modifiable (soft) resource limits
to propagate to the compute nodes and apply to their jobs.  If
\fIrlimits\fR is not specified, then all resource limits will be
propagated.

.TP
\fB\-\-prolog\fR[=]<\fIexecutable\fR>
\fBslaunch\fR will run \fIexecutable\fR just before launching the job step.
The command line arguments for \fIexecutable\fR will be the command
and arguments of the job step.  If \fIexecutable\fR is "none", then
no prolog will be run.  This parameter overrides the SrunProlog
parameter in slurm.conf.

.TP
\fB\-\-epilog\fR[=]<\fIexecutable\fR>
\fBslaunch\fR will run \fIexecutable\fR just after the job step completes.
The command line arguments for \fIexecutable\fR will be the command
and arguments of the job step.  If \fIexecutable\fR is "none", then
no epilog will be run.  This parameter overrides the SrunEpilog
parameter in slurm.conf.

.TP
\fB\-\-task\-prolog\fR[=]<\fIexecutable\fR>
The \fBslurmd\fR daemon will run \fIexecutable\fR just before launching 
each task. This will be executed after any TaskProlog parameter 
in slurm.conf is executed.
Besides the normal environment variables, this has SLURM_TASK_PID
available to identify the process ID of the task being started.
Standard output from this program of the form
"export NAME=value" will be used to set environment variables
for the task being spawned.

.TP
\fB\-\-task\-epilog\fR[=]<\fIexecutable\fR>
The \fBslurmd\fR daemon will run \fIexecutable\fR just after each task
terminates. This will be before after any TaskEpilog parameter      
in slurm.conf is executed. This is meant to be a very short-lived 
program. If it fails to terminate within a few seconds, it will 
be killed along with any descendant processes.

.TP
\fB\-\-ctrl\-comm\-ifhn\fR[=]<\fIaddr\fR>
Specify the address or hostname to be used for PMI communications only
(MPCIH2 communication bootstrapping mechanism).
Defaults to short hostname of the node on which slaunch is running.

.TP 
\fB\-h\fR, \fB\-\-help\fR
Display help information and exit.

.TP
\fB\-u\fR, \fB\-\-usage\fR
Display brief usage message and exit.

.TP 
\fB\-V\fR, \fB\-\-version\fR
Display version information and exit.

.SH "ENVIRONMENT VARIABLES"
.LP 
.TP 
\fBSLAUNCH_JOBID\fP
Same as \fB\-\-jobid\fR.
.SH "EXAMPLES"
.LP 
To launch a job step (parallel program) in an existing job allocation:
.IP 
slaunch \-\-jobid 66777 \-N2 \-n8 myprogram

.LP 
To grab an allocation of nodes and launch a parallel application on one command line (See the \fBsalloc\fR man page for more examples):
.IP 
salloc \-N5 slaunch \-n10 myprogram
.SH "SEE ALSO"
.LP 
sinfo(1), salloc(1), sbatch(1), squeue(1), scancel(1), scontrol(1), slurm.conf(5), sched_setaffinity(2), numa(3)
