#!/usr/bin/env expect
############################################################################
# Purpose: Test of SLURM functionality
#          Ensure gres/mps jobs for same user with different percentages on same
#          node are serialized or run on different GPUs
#
# Output:  "TEST: #.#" followed by "SUCCESS" if test was successful, OR
#          "FAILURE: ..." otherwise with an explanation of the failure, OR
#          anything else indicates a failure mode that must be investigated.
############################################################################
# Copyright (C) 2018 SchedMD LLC
# Written by Morris Jette
#
# This file is part of SLURM, a resource management program.
# For details, see <https://slurm.schedmd.com/>.
# Please also read the included file: DISCLAIMER.
#
# SLURM is free software; you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free
# Software Foundation; either version 2 of the License, or (at your option)
# any later version.
#
# SLURM is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
# details.
#
# You should have received a copy of the GNU General Public License along
# with SLURM; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA.
############################################################################
source ./globals

set test_id     "40.7"
set exit_code   0
set file_in1   "test$test_id.input1"
set file_in2   "test$test_id.input2"
set file_out1  "test$test_id.output1"
set file_out2  "test$test_id.output2"
set file_out3  "test$test_id.output3"
set gpu_index1 -1
set gpu_index2 -1
set gpu_index3 -1
set job_name   "test$test_id"
set job_id1     0
set job_id2     0
set job_id3     0
set sleep_time	10

print_header $test_id

set select_type [test_select_type]
if {![string compare $select_type "cons_tres"] ||
    (![string compare $select_type "cray"] && [test_select_type_params "other_cons_tres"])} {
        send_user "\nValid configuration, using select/cons_tres\n"
} else {
        send_user "\nWARNING: This test is only compatible with select/cons_tres\n"
        exit 0
}

set mps_cnt [get_mps_count 1]
if {$mps_cnt < 0} {
	send_user "\nFAILURE: Error getting MPS count\n"
	exit 1
}
if {$mps_cnt < 1} {
	send_user "\nWARNING: This test requires 1 or more MPS in the default partition\n"
	exit 0
}
if {$mps_cnt < 100} {
	send_user "\nFAILURE: MPS per GPU is unexpectedly low ($mps_cnt < 100). Check your configuration\n"
	set exit_code 1
}
send_user "\nMPS per GPU count is $mps_cnt\n"

make_bash_script $file_in1 "
echo HOST:\$SLURMD_NODENAME
echo CUDA_VISIBLE_DEVICES:\$CUDA_VISIBLE_DEVICES
echo CUDA_MPS_ACTIVE_THREAD_PERCENTAGE:\$CUDA_MPS_ACTIVE_THREAD_PERCENTAGE
$sbatch -w \$SLURMD_NODENAME -n1 -t1 -o $file_out2 -J $job_name --gres=mps:10 $file_in2
$sbatch -w \$SLURMD_NODENAME -n1 -t1 -o $file_out3 -J $job_name --gres=mps:12 $file_in2
sleep $sleep_time
$bin_echo 'RUNNING JOBS:'
$squeue --noheader -t R -n $job_name -o 'JOBID=%i'
$bin_echo 'DETAILS ABOUT THIS JOB:'
$scontrol -dd show job \$SLURM_JOB_ID
sleep $sleep_time
exit 0"

make_bash_script $file_in2 "
echo HOST:\$SLURMD_NODENAME
echo CUDA_VISIBLE_DEVICES:\$CUDA_VISIBLE_DEVICES
echo CUDA_MPS_ACTIVE_THREAD_PERCENTAGE:\$CUDA_MPS_ACTIVE_THREAD_PERCENTAGE
sleep $sleep_time
$bin_echo 'RUNNING JOBS:'
$squeue --noheader -t R -n $job_name -o 'JOBID=%i'
$bin_echo 'DETAILS ABOUT THIS JOB:'
$scontrol -dd show job \$SLURM_JOB_ID
sleep $sleep_time
exit 0"

#
# Main job uses 10 gres/mps and submits 2 additional jobs on the same node
# First job also uses 10 gres/mps and can run on same GPU at the same time
# Second job uses 12 gres/mps and must run on different GPU or at different time
#
exec $bin_rm -f $file_out1 $file_out2 $file_out3
spawn $sbatch -n1 -t1 -o $file_out1 -J $job_name --gres=mps:10 $file_in1
expect {
	-re "Submitted batch job ($number)" {
		set job_id1 $expect_out(1,string)
		exp_continue
	}
	timeout {
                send_user "\nFAILURE: sbatch not responding\n"
		set exit_code 1
	}
	eof {
		wait
	}
}

if {[wait_for_job $job_id1 "DONE"] != 0} {
	send_user "\nFAILURE: job $job_id1 did not complete\n"
	cancel_job $job_id1
	set exit_code 1
}
if {[wait_for_file $file_out1] != 0} {
	send_user "\nFAILURE: no output file\n"
	exit 1
}
set match 0
spawn $bin_cat $file_out1
expect {
	-re "CUDA_VISIBLE_DEVICES:($number)" {
		incr match
		exp_continue
	}
	-re "CUDA_MPS_ACTIVE_THREAD_PERCENTAGE:($number)" {
		incr match
		exp_continue
	}
	-re "Submitted batch job ($number)" {
		incr match
		if {$job_id2 == 0} {
			set job_id2 $expect_out(1,string)
		} else {
			set job_id3 $expect_out(1,string)
		}
		exp_continue
	}
	-re "mps.IDX:($number)" {
		set gpu_index1 $expect_out(1,string)
		exp_continue
	}
	eof {
		wait
	}
}
if {$match != 4} {
	send_user "\nFAILURE: bad output from first job ($match != 4)\n\n"
	set exit_code 1
}

#
# Gather output file information from second job
#
if {[wait_for_job $job_id2 "DONE"] != 0} {
	send_user "\nFAILURE: job $job_id2 did not complete\n"
	cancel_job $job_id2
	set exit_code 1
}
if {[wait_for_file $file_out2] != 0} {
	send_user "\nFAILURE: no output file\n"
	exit 1
}
set match 0
set concurrent_2_1 0
set concurrent_2_3 0
spawn $bin_cat $file_out2
expect {
	-re "CUDA_VISIBLE_DEVICES:($number)" {
		incr match
		exp_continue
	}
	-re "CUDA_MPS_ACTIVE_THREAD_PERCENTAGE:($number)" {
		incr match
		exp_continue
	}
	-re "JOBID=($number)" {
		if {$job_id1 == $expect_out(1,string)} {
			set concurrent_2_1 1
		} elseif {$job_id3 == $expect_out(1,string)} {
			set concurrent_2_3 1
		}
		exp_continue
	}
	-re "mps.IDX:($number)" {
		incr match
		set gpu_index2 $expect_out(1,string)
		exp_continue
	}
	eof {
		wait
	}
}
if {$match != 3} {
	send_user "\nFAILURE: bad output from second job ($match != 3)\n\n"
	set exit_code 1
}

#
# Gather output file information from third job
#
if {[wait_for_job $job_id3 "DONE"] != 0} {
	send_user "\nFAILURE: job $job_id3 did not complete\n"
	cancel_job $job_id3
	set exit_code 1
}
if {[wait_for_file $file_out3] != 0} {
	send_user "\nFAILURE: no output file\n"
	exit 1
}
set match 0
set concurrent_3_1 0
set concurrent_3_2 0
spawn $bin_cat $file_out3
expect {
	-re "CUDA_VISIBLE_DEVICES:($number)" {
		incr match
		exp_continue
	}
	-re "CUDA_MPS_ACTIVE_THREAD_PERCENTAGE:($number)" {
		incr match
		exp_continue
	}
	-re "JOBID=($number)" {
		if {$job_id1 == $expect_out(1,string)} {
			set concurrent_3_1 1
		} elseif {$job_id2 == $expect_out(1,string)} {
			set concurrent_3_2 1
		}
		exp_continue
	}
	-re "mps.IDX:($number)" {
		incr match
		set gpu_index3 $expect_out(1,string)
		exp_continue
	}
	eof {
		wait
	}
}
if {$match != 3} {
	send_user "\nFAILURE: bad output from third job ($match != 3)\n\n"
	set exit_code 1
}

#
# Now check for job concurrency and GPU use.
# Use GPU/MPS index information from "scontrol -dd show job" rather than
# CUDA_VISIBLE_DEVICES environment variable, which will always be zero if
# task/cgroup is used to constrain devices.
#
# Check output from second job
#
if {$concurrent_2_1 == 1} {
	if {$gpu_index2 == $gpu_index2} {
		send_user "\nSecond and first jobs run currently on the same GPU, PERFECT!\n"
	} else {
		send_user "\nSecond and first jobs run currently on the different GPUs, fine\n"
	}
} else {
	send_user "\nSecond and first jobs at different times, fine\n"
}
if {$concurrent_2_3 == 1} {
	if {$gpu_index2 == $gpu_index3} {
		send_user "\nFAILURE: Second and third jobs run currently on the same GPU\n"
		set exit_code 1
	} else {
		send_user "\nSecond and third jobs run currently on the different GPUs, PERFECT!\n"
	}
} else {
	send_user "\nSecond and third jobs at different times, fine\n"
}
#
# Check output from third job
#
if {$concurrent_3_1 == 1} {
	if {$gpu_index3 == $gpu_index1} {
		send_user "\nFAILURE: Third and first jobs run currently on the same GPU\n"
		set exit_code 1
	} else {
		send_user "\nThird and first jobs run currently on the different GPUs, PERFECT!\n"
	}
} else {
	send_user "\nThird and first jobs at different times, fine\n"
}
if {$concurrent_3_2 == 1} {
	if {$gpu_index3 == $gpu_index2} {
		send_user "\nFAILURE: Third and second jobs run currently on the same GPU\n"
		set exit_code 1
	} else {
		send_user "\nThird and second jobs run currently on the different GPUs, PERFECT!\n"
	}
} else {
	send_user "\nThird and second jobs at different times, fine\n"
}

#
# Clean up and exit
#
if {$exit_code == 0} {
	exec $bin_rm -f $file_in1 $file_in2 $file_out1 $file_out2 $file_out3
	send_user "\nSUCCESS\n"
}
exit $exit_code
