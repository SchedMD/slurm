RELEASE NOTES FOR SLURM VERSION 1.3
6 June 2007


IMPORTANT NOTE:
SLURM state files in version 1.3 are different from those of version 1.2.
After installing SLURM version 1.2, plan to restart without preserving 
jobs or other state information. Restart daemons with the "-c" option or 
use "/etc/init.d/slurm startclean".

COMMAND CHANGES

* The srun options --allocate, --attach and --batch have been removed.
  Use the new commands added in SLURM version 1.2 for this functionality:
  salloc  - Create a job allocation (functions like "srun --allocate")
  sattach - Attach to an existing job step (functions like "srun --attach")
  sbatch  - Submit a batch job script (functions like "srun --batch")
  See the individual man pages for more information. 
* The slaunch command has been removed. Use the srun command instead.
* The srun option --exclusive has been added for job steps to be 
  allocated processors not already assigned to other job steps. This 
  can be used to execute multiple job steps simultaneously within a 
  job allocation and have SLURM perform resource management for the 
  job steps much like it does for jobs. If dedicated resources are 
  not immediately available, the job step will be executed later 
  unless the --immediate option is also set.
* Multi-core options have been added to salloc and sbatch commands.  These 
  include -B, --hint, --distribution, --ntasks-per-core, --ntasks-per-socket, 
  and --ntasks-per-thread. In addition the --tasks option in these commands 
  was changed to --ntasks for consistency with the srun command.
* Support is now provided for feature counts in job constraints. For example:
  srun --nodes=16 --constraint=graphics*4 ...
* The srun option --pty has been added to start the job with a pseudo terminal.
* The "scontrol show job" command noe reports the command, arguments, and 
  working directory for batch jobs.
* scontrol now shows job TimeLimit and partition MaxTime in the format of
  [days-]hours:minutes:seconds or "UNLIMITED". The scontrol update options 
  for times now accept minutes, minutes:seconds, hours:minutes:seconds, 
  days-hours, days-hours:minutes, days-hours:minutes:seconds or "UNLIMITED".
* sacct -c can now be used to view job completion data
* scontrol "notify" command added to send message to stdout of srun for 
  specified job id. 
* Support has been added for a much richer job dependency specification 
  including testing of exit codes and multiple dependencies.
* The srun option --checkpoint-path has been added

CONFIGURATION FILE CHANGES

* Added new parameter "PrivateData" which can be used to prevent users 
  from being able to view jobs or job steps belonging to other users.
* Added configuration parameters for node power save mode: ResumeProgram
  ResumeRate, SuspendExcNodes, SuspendExcParts, SuspendProgram and SuspendRate.
* Job accounting has been split into two plugins Job Accounting Gather 
  (jobacct_gather) and Storage (jobacct_storage).  The major jobacct no 
  longer exsists instead use jobacct_gather to specify the gathering method 
  (linux, aix, or none).  jobacct_storage should be used to specify how to
  store the data collected i.e. JobAcctStorageType (filetxt, mysql, and pgsql).
  JobAcctStorageHost, JobAcctStoragePass, JobAcctStoragePort, and 
  JobAcctStorageUser are used to specify how SLURM will talk to the database.
* Job completion also has database hooks built in 
  JobCompHost, JobCompPass, JobCompPort, and JobCompUser are used to 
  specify how SLURM will talk to the database.
* Added new paramter "CryptoType" to specify digital signature plugin to be
  used. Supports crypto/openssl (default) or crypto/munge (for a GPL license).
* Added new parameter "SchedulerTimeSlice" to control the length of gang scheduler
  time slices.
* Added new parameters "DefMemPerTask" and "MaxMemPerTask" any task that exceeds
  the specified size will be terminated.
* Added new parameter "JobRequeue" to control default job behavior after a node 
  failure (requeue or kill the job).
* Added new partition parameter "Priority". A job's scheduling priority is based
  upon two factors. First the priority of its partition and the job's priority. 
  Since nodes can be configured in multiple partitions, this can be used to configure
  high priority partitions (queues).
* The partition parameter "Shared" now has a job count. For example:
  Shared=YES:4     (Up to 4 jobs may share each resource)
  Shared=FORCE:2   (Up to 2 jobs must share each resource)
* The partition MaxTime format now accepts minutes, minutes:seconds, 
  hours:minutes:seconds, days-hours, days-hours:minutes, 
  days-hours:minutes:seconds or "UNLIMITED".
* Checkpoint plugin added for XLCH.
* See "man slurm.conf" for more information.

OTHER CHANGES

* New node state "FAILING" added along with event trigger for it.
* type slurm_step_ctx has changed to slurm_step_ctx_t * make appropriate
  changes.

See the file NEWS for more details.
