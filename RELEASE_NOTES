RELEASE NOTES FOR SLURM VERSION 19.05
16 August 2018

IMPORTANT NOTES:
If using the slurmdbd (Slurm DataBase Daemon) you must update this first.

NOTE: If using a backup DBD you must start the primary first to do any
database conversion, the backup will not start until this has happened.

The 19.05 slurmdbd will work with Slurm daemons of version 17.11 and above.
You will not need to update all clusters at the same time, but it is very
important to update slurmdbd first and having it running before updating
any other clusters making use of it.

Slurm can be upgraded from version 17.11 or 18.08 to version 19.05 without loss
of jobs or other state information. Upgrading directly from an earlier version
of Slurm will result in loss of state information.

If using SPANK plugins that use the Slurm APIs, they should be recompiled when
upgrading Slurm to a new major release.

NOTE: The slurmctld is now set to fatal if there are any problems with
      any state files.  To avoid this use the new '-i' flag.

NOTE: systemd services files are installed automatically, but not enabled.
      You will need to manually enable them on the appropriate systems:
      - Controller: systemctl enable slurmctld
      - Database: systemctl enable slurmdbd
      - Compute Nodes: systemctl enable slurmd

NOTE: Cray/ALPS support has been removed.

NOTE: Built-in BLCR support has been removed.

NOTE: The proctrack/sgi_job plugin has been removed.

NOTE: slurmd and slurmctld will now fatal if two incompatible mechanisms for
      enforcing memory limits are set. This makes incompatible the use of
      task/cgroup memory limit enforcing (Constrain[RAM|Swap]Space=yes) with
      MemLimitEnforce=yes or with JobAcctGatherParams=OverMemoryKill, which
      could cause problems when a task is killed by one of them while the other
      is at the same time managing that task. The NoOverMemoryKill setting has
      been deprecated in favour of OverMemoryKill, since now the default is
      *NOT* to have any memory enforcement mechanism.

NOTE: SLURM_FAILURE, SLURM_SOCKET_ERROR, SLURM_PROTOCOL_SUCCESS, and
      SLURM_PROTOCOL_ERROR been removed please update to SLURM_SUCCESS or
      SLURM_ERROR as appropriate.

HIGHLIGHTS
==========
 -- Add select/cons_tres plugin, which offers similar functionality to cons_res
    with an entirely new code base and far greater GPU scheduling flexibility.
 -- Remove select/serial plugin.

RPMBUILD CHANGES
================

CONFIGURATION FILE CHANGES (see man appropriate man page for details)
=====================================================================
 -- Add GPU scheduling options to slurm.conf, available both globally and
    per-partition: DefCpusPerGPU and DefMemPerGPU.

COMMAND CHANGES (see man pages for details)
===========================================
 -- Add GPU scheduling options for salloc, sbatch and srun:
   --cpus-per-gpu, -G/--gpus, --gpu-bind, --gpu-freq, --gpus-per-node,
   --gpus-per-socket, --gpus-per-task and --mem-per-gpu.
 -- If GRES are associated with specific sockets, identify those sockets in the
    output of "scontrol show node" (e.g. "Gres=gpu:4(S:0)").

OTHER CHANGES
=============
