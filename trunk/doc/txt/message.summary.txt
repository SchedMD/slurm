SLURM message traffic summary

Below is a very terse outline of possible message traffic that may occur
across the various components of slurm. These messages will most likely
form a REQ/REPLY pair, where - following Moe's convention - the Input is
contained in the `request' message, and the Output will be found in the
`reply.'

Command(s):	Get job information, separate commands for 
          	accounting, node, partition, job step and build info
Client:   	squeue and scontrol commands, plus DPCS from API, any node in cluster
Server: 	slurmctld
Input:  	time-stamp, version, user id
		flags	: might be useful for filtering data sent, e.g. just this user's jobs 
Output: 	error code, version, time-stamp, record count, array of records
Notes:  	most information generally available, some might be restricted by user id


Command(s):	Get key
Client:  	API call (used by DPCS)
Server: 	slurmctld
Input:  	uid (must be root)
Output: 	key
Notes:   	used to control access to some partitions. for example, any user 
		can run jobs in the "batch" partition, but only when initiated by 
		a batch controller (e.g. DPCS). this prevents users from running 
		jobs outside of the queue structure


Command(s):	Allocate 
Client:   	srun or slurm api call
Server: 	slurmctld 
Input:		username/uid,nnodes,ntasks, group
		optional: partition,time_limit,constraints,features,node list, key
		flags   : wait_for_resources, test only (don't allocate resources, 
		          just reply whether or not allocate would have succeeded, 
		          used by DPCS)
Output: 	jobid, return code, error code, node list, ncpus for *each* node in list
Notes:  	allocate resources to a ``job''


Command(s):	Submit
Client:   	srun or slurm api call
Server: 	slurmctld
Input:  	Allocate input + script path, environment, cwd 
                optional: partition, time_limit, constraints, features, 
		          I/O location, signal handling, key
		flags:
Output: 	jobid, return code, error code
Notes:  	submit a batch job to the slurm queue


Command(s):	Run Job Step	
Client:   	srun or slurm api call
Server: 	slurmctld
Input:  	jobid,username/uid
		optional: nnodes,ntasks,cpus_per_task,distribution,time_limit,
		          constraints,features,signal handling
		flags   : wait_for_resources
Output: 	jobid, stepid, return code, error code, node list, ncpus/node
 		credential list,
Notes:  	run a set of parallel tasks under an allocated job
 		allocate resources if jobid < MIN_JOBID, otherwise assume 
		resources are already available


Command(s):	Job Resource Request
Client:   	srun, scancel 
Server: 	slurmctld
Input:  	stepid
Output: 	return code, error code, node list, ncpus/node, credentials
Notes:  	obtain a new set of credentials for a job. Needed for
 		at least `srun --attach`
	
	
Command(s):	Run Job Request
Client:   	srun or slurmctld
Server: 	slurmd
Input:  	username/uid, jobid, stepid, credential, ntasks, environment, 
		cwd, command line, stdin location, stdout/err location
Output: 	return code, error code
Notes:  	request initiation of ntasks tasks on this node.


Command(s):	Signal Job Request
Client:   	srun or slurmctld (possibly scancel)
Server: 	slurmd
Input:  	uid, jobid or stepid, signal no.
		optional: task no.
Output: 	return code
Notes:  	


Command(s):	Kill Job Request
Client:   	srun or slurmctld (possibly scancel)
Server: 	slurmd
Input:  	uid, jobid or stepid
Output: 	return code
Notes:  	explicitly kill job as opposed to implicit job kill
		with a signal job request.


Command(s):	Job Attach Request
Client:   	srun
Server: 	slurmd
Input:  	uid, stepid
Output: 	return code, error code, 
		stdout/err duplicated to srun stdout/err, signals propagated,
Notes:  	srun process ``attaches'' to a currently running job. This
		request is used for srun recovery, or by a user who wants
		to interactively reattach to a batch job.


Command(s):	Cancel job or allocation
Client:   	scancel user command, plus DPCS from API, any node in cluster
Server: 	slurmctld
Input:  	user id, jobid or stepid
Output: 	error code
Notes:  	can only be run as user root or the user id for the job


Command(s):	Reconfigure (tell slurmctld to re-read configuration)
Client:   	scontrol administrator command, any node in cluster
Server: 	slurmctld
Input:  	user id (root), version
Output: 	error code, version
Notes:  	can only be run as user root


Command(s):	Register node (slurmd starting)
Client:   	slurmd daemon, any node in cluster
Server: 	slurmctld
Input:  	version, time stamp, processor count, memory size, temporary disk space
Output: 	none
Notes:  	


Command(s):	Status node
Client:   	slurmctld or backup slurmctld
Server: 	slurmd or slurmctld (for backup check) daemon, any node in cluster
Input:  	none
Output: 	version, time stamp, processor count, memory size, temporary disk space
Notes:  	


Command(s):	Upload accounting information
Client:   	slurmctld
Server: 	slurmd daemon, any node in cluster
Input:  	none
Output: 	version, time stamp, collection of records with user id, memory use, 
		CPU time used, CPU time allocated, etc.
Notes:  	not needed for initial release


Command(s):	Get job id from process id
Client:   	DPCS API
Server: 	slurmd daemon on the same node as DPCS API is executed
Input:  	process id
Output: 	SLURM job id
Notes:  	until SLURM accounting is fully funcational, DPCS needs help figuring 
		out what processes are associated with each job


Command(s):	Create job step
Client:   	srun from user script, any node in cluster
Server: 	slurmctld
Input:  	job id, uid, node list, task distribution, processors per task
Output: 	step id, elan context (opaque data structure)
Notes:  	needed to start parallel program


Command(s):	Get job step infomration
Client:   	srun from user script, slurmd, any node in cluster
Server: 	slurmctld
Input:  	job id, uid, step id
Output: 	step id, elan context (opaque data structure)
Notes:  	needed to start parallel program


Command(s):	Modify node information
Client:   	scontrol
Server: 	slurmctld
Input:  	node name, node state, uid
Output: 	exit code
Notes:  	Only the node state can be changed (esp. to DRAINING, DOWN, IDLE).
		Only request from user root is accepted

Command(s):	Modify partition information
Client:   	scontrol
Server: 	slurmctld
Input:  	partition name, allowed groups, default, key, max time, max nodes, 
		node list, shared, state, uid
Output: 	exit code
Notes:  	Only request from user root is accepted.


Command(s):	Modify job information
Client:   	scontrol
Server: 	slurmctld
Input:  	job id, time limit, priority, uid (must be root or owner
Output: 	exit code
Notes:  	Only request from the job's owner and user root are accepted.

----TEMPLATE----
Command(s):	
Client:   	
Server: 	
Input:  	
Output: 	
Notes:  	
