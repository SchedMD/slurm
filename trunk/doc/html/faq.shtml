<!--#include virtual="header.txt"-->

<h1><a name="top">Frequently Asked Questions</a></h1>
<h2>For Users</h2>
<ol>
<li><a href="#comp">Why is my job/node in COMPLETING state?</a></li>
<li><a href="#rlimit">Why do I see the error &quot;Can't propagate RLIMIT_...&quot;?</a></li>
<li><a href="#pending">Why is my job not running?</a></li>
<li><a href="#sharing">Why does the srun --overcommit option not permit multiple jobs 
to run on nodes?</a></li>
<li><a href="#purge">Why is my job killed prematurely?</a></li>
<li><a href="#opts">Why are my srun options ignored?</a></li>
<li><a href="#cred">Why are &quot;Invalid job credential&quot; errors generated?</a></li>
<li><a href="#backfill">Why is the SLURM backfill scheduler not starting my 
job?</a></li>
<li><a href="#steps">How can I run multiple jobs from within a single script?</a></li>
<li><a href="#orphan">Why do I have job steps when my job has already COMPLETED?</a></li>
</ol>
<h2>For Administrators</h2>
<ol>
<li><a href="#suspend">How is job suspend/resume useful?</a></li>
<li><a href="#fast_schedule">How can I configure SLURM to use the resources actually 
found on a node rather than what is defined in <i>slurm.conf</i>?</a></li>
<li><a href="#return_to_service">Why is a node shown in state DOWN when the node 
has registered for service?</a></li>
<li><a href="#down_node">What happens when a node crashes?</a></li>
<li><a href="#multi_job">How can I control the execution of multiple 
jobs per node?</a></li>
<li><a href="#inc_plugin">When the SLURM daemon starts, it prints
&quot;cannot resolve X plugin operations&quot; and exits. What does this mean?</a></li>
<li><a href="#sigpipe">Why are user tasks intermittently dying at launch with SIGPIPE
error messages?</a></li>
<li><a href="#maint_time">How can I dry up the workload for a maintenance 
period?</a></li>
<li><a href="#pam">How can PAM be used to control a user's limits on or 
access to compute nodes?</a></li>
<li><a href="#time">Why are jobs allocated nodes and then unable to initiate 
programs on some nodes?</a></li>
<li><a href="#ping"> Why does <i>slurmctld</i> log that some nodes
are not responding even if they are not in any partition?</a></li>
<li><a href="#controller"> How should I relocated the primary or backup
controller?</a></li>
<li><a href="#multi_slurm">Can multiple SLURM systems be run in 
parallel for testing purposes?</a></li>
<li><a href="#multi_slurmd">Can multiple slurmd daemons be run
on the compute node(s) to emulate a larger cluster?</a></li>
</ol>

<h2>For Users</h2>
<p><a name="comp"><b>1. Why is my job/node in COMPLETING state?</b></a><br>
When a job is terminating, both the job and its nodes enter the COMPLETING state. 
As the SLURM daemon on each node determines that all processes associated with 
the job have terminated, that node changes state to IDLE or some other appropriate 
state. 
When every node allocated to a job has determined that all processes associated 
with it have terminated, the job changes state to COMPLETED or some other 
appropriate state (e.g. FAILED). 
Normally, this happens within a second. 
However, if the job has processes that cannot be terminated with a SIGKILL
signal, the job and one or more nodes can remain in the COMPLETING state 
for an extended period of time. 
This may be indicative of processes hung waiting for a core file 
to complete I/O or operating system failure. 
If this state persists, the system administrator should check for processes 
associated with the job that can not be terminated then use the 
<span class="commandline">scontrol</span> command to change the node's 
state to DOWN (e.g. &quot;scontrol update NodeName=<i>name</i> State=DOWN Reason=hung_completing&quot;), 
reboot the node, then reset the node's state to IDLE 
(e.g. &quot;scontrol update NodeName=<i>name</i> State=RESUME&quot;).
Note that setting the node DOWN will terminate all running or suspended 
jobs associated with that node. 
An alternative is to set the node's state to DRAIN until all jobs 
associated with it terminate before setting it DOWN and re-booting.</p>

<p><a name="rlimit"><b>2. Why do I see the error &quot;Can't propagate RLIMIT_...&quot;?</b></a><br>
When the <span class="commandline">srun</span> command executes, it captures the 
resource limits in effect at that time. These limits are propagated to the allocated 
nodes before initiating the user's job. If the soft resource limits on the job 
submit host are higher than the hard resource limits on the allocated host, SLURM 
will be unable to propagate the resource limits and print an error of the type 
shown above. It is recommended that the system administrator establish uniform 
hard resource limits on all nodes within a cluster to prevent this from occurring.</p>

<p><a name="pending"><b>3. Why is my job not running?</b></a><br>
The answer to this question depends upon the scheduler used by SLURM. Executing 
the command</p>
<blockquote> 
<p> <span class="commandline">scontrol show config | grep SchedulerType</span></p>
</blockquote>
<p> will supply this information. If the scheduler type is <b>builtin</b>, then 
jobs will be executed in the order of submission for a given partition. Even if 
resources are available to initiate your job immediately, it will be deferred 
until no previously submitted job is pending. If the scheduler type is <b>backfill</b>, 
then jobs will generally be executed in the order of submission for a given partition 
with one exception: later submitted jobs will be initiated early if doing so does 
not delay the expected execution time of an earlier submitted job. In order for 
backfill scheduling to be effective, users jobs should specify reasonable time 
limits. If jobs do not specify time limits, then all jobs will receive the same 
time limit (that associated with the partition), and the ability to backfill schedule 
jobs will be limited. The backfill scheduler does not alter job specifications 
of required or excluded nodes, so jobs which specify nodes will substantially 
reduce the effectiveness of backfill scheduling. See the <a href="#backfill">
backfill</a> section for more details. If the scheduler type is <b>wiki</b>, 
this represents 
<a href="http://www.clusterresources.com/pages/products/maui-cluster-scheduler.php">
The Maui Scheduler</a> or 
<a href="http://www.clusterresources.com/pages/products/moab-cluster-suite.php">
Moab Cluster Suite</a>. 
Please refer to its documentation for help. For any scheduler, you can check priorities 
of jobs using the command <span class="commandline">scontrol show job</span>.</p>
<p><a name="sharing"><b>4. Why does the srun --overcommit option not permit multiple jobs 
to run on nodes?</b></a><br>
The <b>--overcommit</b> option is a means of indicating that a job or job step is willing 
to execute more than one task per processor in the job's allocation. For example, 
consider a cluster of two processor nodes. The srun execute line may be something 
of this sort 
<blockquote>
<p><span class="commandline">srun --ntasks=4 --nodes=1 a.out</span></p>
</blockquote>
This will result in not one, but two nodes being allocated so that each of the four 
tasks is given its own processor. Note that the srun <b>--nodes</b> option specifies 
a minimum node count and optionally a maximum node count. A command line of
<blockquote>
<p><span class="commandline">srun --ntasks=4 --nodes=1-1 a.out</span></p>
</blockquote>
would result in the request being rejected. If the <b>--overcommit</b> option 
is added to either command line, then only one node will be allocated for all 
four tasks to use.
<p>More than one job can execute simultaneously on the same nodes through the use 
of srun's <b>--shared</b> option in conjunction with the <b>Shared</b> parameter 
in SLURM's partition configuration. See the man pages for srun and slurm.conf for 
more information.
<p><a name="purge"><b>5. Why is my job killed prematurely?</b></a><br>
SLURM has a job purging mechanism to remove inactive jobs (resource allocations)
before reaching its time limit, which could be infinite.
This inactivity time limit is configurable by the system administrator. 
You can check it's value with the command
<blockquote>
<p><span class="commandline">scontrol show config | grep InactiveLimit</span></p>
</blockquote>
The value of InactiveLimit is in seconds. 
A zero value indicates that job purging is disabled. 
A job is considered inactive if it has no active job steps or if the srun 
command creating the job is not responding.
In the case of a batch job, the srun command terminates after the job script 
is submitted. 
Therefore batch job pre- and post-processing is limited to the InactiveLimit.
Contact your system administrator if you believe the InactiveLimit value 
should be changed. 

<p><a name="opts"><b>6. Why are my srun options ignored?</b></a><br>
Everything after the command <span class="commandline">srun</span> is 
examined to determine if it is a valid option for srun. The first 
token that is not a valid option for srun is considered the command 
to execute and everything after that is treated as an option to 
the command. For example:
<blockquote>
<p><span class="commandline">srun -N2 hostname -pdebug</span></p>
</blockquote>
srun processes "-N2" as an option to itself. "hostname" is the 
command to execute and "-pdebug" is treated as an option to the 
hostname command. Which will change the name of the computer 
on which SLURM executes the command - Very bad, <b>Don't run 
this command as user root!</b></p>

<p><a name="cred"><b>7. Why are &quot;Invalid job credential&quot; errors generated?
</b></a><br>
This error is indicative of SLURM's job credential files being inconsistent across 
the cluster. All nodes in the cluster must have the matching public and private 
keys as defined by <b>JobCredPrivateKey</b> and <b>JobCredPublicKey</b> in the 
slurm configuration file <b>slurm.conf</b>.

<p><a name="backfill"><b>8. Why is the SLURM backfill scheduler not starting my job?
</b></a><br>
There are significant limitations in the current backfill scheduler plugin. 
It was designed to perform backfill node scheduling for a homogeneous cluster.
It does not manage scheduling on individual processors (or other consumable 
resources). It also does not update the required or excluded node list of 
individual jobs. These are the current limiations. You can use the 
scontrol show command to check if these conditions apply. 
<ul>
<li>partition: State=UP</li>
<li>partition: RootOnly=NO</li>
<li>partition: Shared=NO</li>
<li>job: ReqNodeList=NULL</li>
<li>job: ExcNodeList=NULL</li>
<li>job: Contiguous=0</li>
<li>job: Features=NULL</li>
<li>job: MinProcs, MinMemory, and MinTmpDisk satisfied by all nodes in 
the partition</li>
<li>job: MinProcs or MinNodes not to exceed partition's MaxNodes</li>
</ul>
As soon as any priority-ordered job in the partition's queue fail to 
satisfy the request, no lower priority job in that partition's queue 
will be considered as a backfill candidate. Any programmer wishing 
to augment the existing code is welcome to do so. 

<p><a name="steps"><b>9. How can I run multiple jobs from within a 
single script?</b></a><br>
A SLURM job is just a resource allocation. You can execute many 
job steps within that allocation, either in parallel or sequentially. 
Some jobs actually launch thousands of job steps this way. The job 
steps will be allocated nodes that are not already allocated to 
other job steps. This essential provides a second level of resource 
management within the job for the job steps.</p>

<p><a name="orphan"><b>10. Why do I have job steps when my job has 
already COMPLETED?</b></a><br>
NOTE: This only applies to systems configured with 
<i>SwitchType=switch/elan</i> or <i>SwitchType=switch/federation</i>.
All other systems will purge all job steps on job completion.</p>
<p>SLURM maintains switch (network interconnect) information within 
the job step for Quadrics Elan and IBM Federation switches. 
This information must be maintained until we are absolutely certain 
that the processes associated with the switch have been terminated 
to avoid the possibility of re-using switch resources for other 
jobs (even on different nodes).
SLURM considers jobs COMPLETED when all nodes allocated to the 
job are either DOWN or confirm termination of all it's processes.
This enables SLURM to purge job information in a timely fashion 
even when there are many failing nodes.
Unfortunately the job step information may persist longer.</p>

<p class="footer"><a href="#top">top</a></p>

<h2>For Administrators</h2>
<p><a name="suspend"><b>1. How is job suspend/resume useful?</b></a><br>
Job suspend/resume is most useful to get particularly large jobs initiated 
in a timely fashion with minimal overhead. Say you want to get a full-system
job initiated. Normally you would need to either cancel all running jobs 
or wait for them to terminate. Canceling jobs results in the loss of 
their work to that point from either their beginning or last checkpoint.
Waiting for the jobs to terminate can take hours, depending upon your
system configuration. A more attractive alternative is to suspend the 
running jobs, run the full-system job, then resume the suspended jobs. 
This can easily be accomplished by configuring a special queue for 
full-system jobs and using a script to control the process. 
The script would stop the other partitions, suspend running jobs in those 
partitions, and start the full-system partition. 
The process can be reversed when desired.  
One can effectively gang schedule (time-slice) multiple jobs 
using this mechanism, although the algorithms to do so can get quite 
complex.
Suspending and resuming a job makes use of the SIGSTOP and SIGCONT 
signals respectively, so swap and disk space should be sufficient to 
accommodate all jobs allocated to a node, either running or suspended.

<p><a name="fast_schedule"><b>2. How can I configure SLURM to use 
the resources actually found on a node rather than what is defined 
in <i>slurm.conf</i>?</b></a><br>
SLURM can either base it's scheduling decisions upon the node 
configuration defined in <i>slurm.conf</i> or what each node 
actually returns as available resources. 
This is controlled using the configuration parameter <i>FastSchedule</i>.
Set it's value to zero in order to use the resources actually 
found on each node, but with a higher overhead for scheduling.
A value of one is the default and results in the node configuration 
defined in <i>slurm.conf</i> being used. See &quot;man slurm.conf&quot;
for more details.</p>

<p><a name="return_to_service"><b>3. Why is a node shown in state 
DOWN when the node has registered for service?</b></a><br>
The configuration parameter <i>ReturnToService</i> in <i>slurm.conf</i>
controls how DOWN nodes are handled. 
Set its value to one in order for DOWN nodes to automatically be 
returned to service once the <i>slurmd</i> daemon registers 
with a valid node configuration.
A value of zero is the default and results in a node staying DOWN 
until an administrator explicity returns it to service using 
the command &quot;scontrol update NodeName=whatever State=RESUME&quot;.
See &quot;man slurm.conf&quot; and &quot;man scontrol&quot; for more 
details.</p>

<p><a name="down_node"><b>4. What happens when a node crashes?</b></a><br>
A node is set DOWN when the slurmd daemon on it stops responding 
for <i>SlurmdTimeout</i> as defined in <i>slurm.conf</i>.
The node can also be set DOWN when certain errors occur or the 
node's configuration is inconsistent with that defined in <i>slurm.conf</i>.
Any active job on that node will be killed unless it was submitted 
with the srun option <i>--no-kill</i>.
Any active job step on that node will be killed. 
See the slurm.conf and srun man pages for more information.</p>
 
<p><a name="multi_job"><b>5. How can I control the execution of multiple 
jobs per node?</b></a><br>
There are two mechanism to control this. 
If you want to allocate individual processors on a node to jobs, 
configure <i>SelectType=select/cons_res</i>. 
See <a href="cons_res.html">Consumable Resources in SLURM</a>
for details about this configuration.  
If you want to allocate whole nodes to jobs, configure
configure <i>SelectType=select/linear</i>.
Each partition also has a configuration parameter <i>Shared</i>
that enables more than one job to execute on each node. 
See <i>man slurm.conf</i> for more information about these 
configuration paramters.</p>

<p><a name="inc_plugin"><b>6. When the SLURM daemon starts, it 
prints &quot;cannot resolve X plugin operations&quot; and exits. 
What does this mean?</b></a><br>
This means that symbols expected in the plugin were 
not found by the daemon. This typically happens when the 
plugin was built or installed improperly or the configuration 
file is telling the plugin to use an old plugin (say from the 
previous version of SLURM). Restart the daemon in verbose mode 
for more information (e.g. &quot;slurmctld -Dvvvvv&quot;). 

<p><a name="sigpipe"><b>7. Why are user tasks intermittently dying
at launch with SIGPIPE error messages?</b></a><br>
If you are using ldap or some other remote name service for
username and groups lookup, chances are that the underlying
libc library functions are triggering the SIGPIPE.  You can likely
work around this problem by setting <i>CacheGroups=1</i> in your slurm.conf
file.  However, be aware that you will need to run &quot;scontrol
reconfigure &quot; any time your groups database is updated.

<p><a name="maint_time"><b>8. How can I dry up the workload for a 
maintenance period?</b></a><br>
There isn't a mechanism to tell SLURM that all jobs should be 
completed by a specific time. The best way to address this is 
to shorten the <i>MaxTime</i> associated with the partitions so 
as to avoid initiating jobs that will not have completed by 
the maintenance period.

<p><a name="pam"><b>9. How can PAM be used to control a user's limits on 
or access to compute nodes?</b></a><br>
First, enable SLURM's use of PAM by setting <i>UsePAM=1</i> in 
<i>slurm.conf</i>.<br>
Second, establish a PAM configuration file for slurm in <i>/etc/pam.d/slurm</i>.
A basic configuration you might use is:<br>
<pre>
auth     required  pam_localuser.so
account  required  pam_unix.so
session  required  pam_limits.so
</pre>
Third, set the desired limits in <i>/etc/security/limits.conf</i>.
For example, to set the locked memory limit to unlimited for all users:<br>
<pre>
*   hard   memlock   unlimited
*   soft   memlock   unlimited
</pre>
Finally, you need to disable SLURM's forwarding of the limits from the 
session from which the <i>srun</i> initiating the job ran. By default 
all resource limits are propogated from that session. For example, adding 
the following line to <i>slurm.conf</i> will prevent the locked memory 
limit from being propagated:<i>PropagateResourceLimitsExcept=MEMLOCK</i>.

<p>We also have a PAM module for SLURM that prevents users from 
logging into nodes that they have not been allocated (except for user 
root, which can always login. pam_slurm is available for download from
<a href="ftp://ftp.llnl.gov/pub/linux/pam_slurm/">ftp://ftp.llnl.gov/pub/linux/pam_slurm</a>
The use of pam_slurm does not require <i>UsePAM</i> being set. The 
two uses of PAM are independent.

<p><a name="time"><b>10. Why are jobs allocated nodes and then unable 
to initiate programs on some nodes?</b></a><br>
This typically indicates that the time on some nodes is not consistent 
with the node on which the <i>slurmctld</i> daemon executes. In order to 
initiate a job step (or batch job), the <i>slurmctld</i> daemon generates 
a credential containing a time stamp. If the <i>slurmd</i> daemon 
receives a credential containing a time stamp later than the current 
time or more than a few minutes in the past, it will be rejected. 
If you check in the <i>SlurmdLog</i> on the nodes of interest, you 
will likely see messages of this sort: "<i>Invalid job credential from 
&lt;some IP address&gt;: Job credential expired</i>." Make the times 
consistent across all of the nodes and all should be well.

<p><a name="ping"><b>11. Why does <i>slurmctld</i> log that some nodes 
are not responding even if they are not in any partition?</b></a><br>
The <i>slurmctld</i> daemon periodically pings the <i>slurmd</i> 
daemon on every configured node, even if not associated with any 
partition. You can control the frequency of this ping with the 
<i>SlurmdTimeout</i> configuration parameter in <i>slurm.conf</i>.

<p><a name="controller"><b>12. How should I relocated the primary or 
backup controller?</b></a><br>
If the cluster's computers used for the primary or backup controller
will be out of service for an extended period of time, it may be desirable
to relocate them. In order to do so, follow this procedure:
<ol>
<li>Stop all SLURM daemons</li>
<li>Modify the <i>ControlMachine</i>, <i>ControlAddr</i>, 
<i>BackupController</i>, and/or <i>BackupAddr</i> in the <i>slurm.conf</i> file</li>
<li>Distribute the updated <i>slurm.conf</i> file file to all nodes</li>
<li>Restart all SLURM daemons</li>
</ol>
There should be no loss of any running or pending jobs. Insure that
any nodes added to the cluster have a current <i>slurm.conf</i> file
installed.
<b>CAUTION:</b> If two nodes are simultaneously configured as the primary 
controller (two nodes on which <i>ControlMachine</i> specify the local host 
and the <i>slurmctld</i> daemon is executing on each), system behavior will be
destructive. If a compute node has an incorrect <i>ControlMachine</i> or
<i>BackupController</i> parameter, that node may be rendered unusable, but no
other harm will result.

<p><a name="multi_slurm"><b>13. Can multiple SLURM systems be run in 
parallel for testing purposes?</b></a><br>
Yes, this is a great way to test new versions of SLURM.
Just install the test version in a different location with a different 
<i>slurm.conf</i>. 
The test system's <i>slurm.conf</i> should specify different 
pathnames and port numbers to avoid conflicts.
The only problem is if more than one version of SLURM is configured 
with <i>switch/elan</i> or <i>switch/federation</i>.
In that case, there can be conflicting switch window requests from 
the different SLURM systems. 
This can be avoided by configuring the test system with <i>switch/none</i>.
MPI jobs started on an Elan or Federation switch system without the 
switch windows configured will not execute properly, but other jobs 
will run fine. 
Another option for testing on Elan or Federation systems is to use 
a different set of nodes for the different SLURM systems.
That will permit both systems to allocate switch windows without 
conflicts.

<p><a name="multi_slurmd"><b>14. Can multiple slurmd daemons be run 
on the compute node(s) to emulate a larger cluster?</b></a><br>
Yes, this can be useful for testing purposes. 
It has also been used to partition "fat" nodes into multiple SLURM nodes.
<ol>
<li>When executing the <i>configure</i> program, use the option 
<i>--multiple-slurmd</i> (or add that option to your <i>~/.rpmmacros</i>
file).</li>
<li>Build and install SLURM in the usual manner.</li>
<li>In <i>slurm.conf</i> define the desired node names (arbitrary 
names used only by SLURM) as <i>NodeName</i> along with the actual
address of the physical node in <i>NodeHostname</i>. Multiple 
<i>NodeName</i> values can be mapped to a single
<i>NodeHostname</i>.  Note that each <i>NodeName</i> on a single
physical node needs to be configured to use a different port number.  You
will also want to use the "%n" symbol in slurmd related path options in
slurm.conf. </li>
<li>When starting the <i>slurmd</i> daemon, include the <i>NodeName</i>
of the node that it is supposed to serve on the execute line.</li> 
</ol>
It is strongly recommended that SLURM version 1.2 or higher be used 
for this due to it's improved support for multiple slurmd daemons.

See the
<a href="programmer_guide.shtml#multiple_slurmd_support">Programmers Guide</a>
for more details about configuring multiple slurmd support.

<p class="footer"><a href="#top">top</a></p>

<p style="text-align:center;">Last modified 9 November 2006</p>

<!--#include virtual="footer.txt"-->
